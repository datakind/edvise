resources:
  jobs:
    edvise_github_sourced_pdp_inference_pipeline:
      name: edvise_github_sourced_pdp_inference_pipeline
      max_concurrent_runs: 2
      tasks:
        - task_key: data_ingestion
          spark_python_task:
            python_file: src/edvise/scripts/pdp_gcp_databricks_ingestion.py
            source: GIT
            parameters:
              - --cohort_file_name
              - "{{job.parameters.cohort_file_name}}"
              - --course_file_name
              - "{{job.parameters.course_file_name}}"
              - --databricks_institution_name
              - "{{job.parameters.databricks_institution_name}}"
              - --db_run_id
              - "{{job.parameters.db_run_id}}"
              - --DB_workspace
              - "{{job.parameters.DB_workspace}}"
              - --gcp_bucket_name
              - "{{job.parameters.gcp_bucket_name}}"
          job_cluster_key: edvise-pdp-inference-pipeline-cluster
          libraries:
            - pypi:
                package: pandas==2.2.3
            - pypi:
                package: google-cloud-storage==2.19.0
        - task_key: data_audit
          depends_on:
            - task_key: data_ingestion
          spark_python_task:
            python_file: src/edvise/scripts/pdp_data_audit.py
            source: GIT
            parameters:
              - --bronze_volume_path
              - "/Volumes/{{job.parameters.DB_workspace}}/{{job.parameters.databricks_institution_name}}_bronze/bronze_volume"
              - --silver_volume_path
              - "/Volumes/{{job.parameters.DB_workspace}}/{{job.parameters.databricks_institution_name}}_silver/silver_volume"
              - --DB_workspace
              - "{{job.parameters.DB_workspace}}"
              - --config_file_path 
              - "/Volumes/{{job.parameters.DB_workspace}}/{{job.parameters.databricks_institution_name}}_bronze/bronze_volume/training_inputs/{{job.parameters.config_file_name}}"
              - --job_type
              - "{{job.parameters.job_type}}"
          job_cluster_key: edvise-pdp-inference-pipeline-cluster
          libraries:
            - pypi:
                package: pandas==2.2.3
            - pypi:
                package: numpy==1.26.4
            - pypi:
                package: faker==33.1.0
            - pypi:
                package: google-cloud-storage==2.19.0
            - pypi:
                package: missingno==0.5.2
            - pypi:
                package: tomli==2.2.1
            - pypi:
                package: tomlkit==0.13.3
            - pypi:
                package: pandera==0.23.0
            - pypi:
                package: pydantic~=2.10
        - task_key: feature_generation
          depends_on:
            - task_key: data_audit
          spark_python_task:
            python_file: src/edvise/scripts/pdp_feature_generation.py
            source: GIT
            parameters:
              - --silver_volume_path
              - "/Volumes/{{job.parameters.DB_workspace}}/{{job.parameters.databricks_institution_name}}_silver/silver_volume"
              - --config_file_path 
              - "/Volumes/{{job.parameters.DB_workspace}}/{{job.parameters.databricks_institution_name}}_bronze/bronze_volume/training_inputs/{{job.parameters.config_file_name}}"
          job_cluster_key: edvise-pdp-inference-pipeline-cluster
          libraries:
            - pypi:
                package: pandas==2.2.3
            - pypi:
                package: numpy==1.26.4
            - pypi:
                package: faker==33.1.0
            - pypi:
                package: google-cloud-storage==2.19.0
            - pypi:
                package: missingno==0.5.2
            - pypi:
                package: tomli==2.2.1
            - pypi:
                package: tomlkit==0.13.3
            - pypi:
                package: pandera==0.23.0
            - pypi:
                package: pydantic~=2.10
        - task_key: checkpoint
          depends_on:
            - task_key: feature_generation
          spark_python_task:
            python_file: src/edvise/scripts/pdp_checkpoints.py
            source: GIT
            parameters:
              - --silver_volume_path
              - "/Volumes/{{job.parameters.DB_workspace}}/{{job.parameters.databricks_institution_name}}_silver/silver_volume"
              - --config_file_path 
              - "/Volumes/{{job.parameters.DB_workspace}}/{{job.parameters.databricks_institution_name}}_bronze/bronze_volume/training_inputs/{{job.parameters.config_file_name}}"
          job_cluster_key: edvise-pdp-inference-pipeline-cluster
          libraries:
            - pypi:
                package: pandas==2.2.3
            - pypi:
                package: numpy==1.26.4
            - pypi:
                package: faker==33.1.0
            - pypi:
                package: google-cloud-storage==2.19.0
            - pypi:
                package: missingno==0.5.2
            - pypi:
                package: tomli==2.2.1
            - pypi:
                package: tomlkit==0.13.3
            - pypi:
                package: pandera==0.23.0
            - pypi:
                package: pydantic~=2.10
        - task_key: student_selection
          depends_on:
            - task_key: feature_generation
          spark_python_task:
            python_file: src/edvise/scripts/pdp_student_selection.py
            source: GIT
            parameters:
              - --silver_volume_path
              - "/Volumes/{{job.parameters.DB_workspace}}/{{job.parameters.databricks_institution_name}}_silver/silver_volume"
              - --config_file_path 
              - "/Volumes/{{job.parameters.DB_workspace}}/{{job.parameters.databricks_institution_name}}_bronze/bronze_volume/training_inputs/{{job.parameters.config_file_name}}"
          job_cluster_key: edvise-pdp-inference-pipeline-cluster
          libraries:
            - pypi:
                package: pandas==2.2.3
            - pypi:
                package: numpy==1.26.4
            - pypi:
                package: faker==33.1.0
            - pypi:
                package: google-cloud-storage==2.19.0
            - pypi:
                package: missingno==0.5.2
            - pypi:
                package: tomli==2.2.1
            - pypi:
                package: tomlkit==0.13.3
            - pypi:
                package: pandera==0.23.0
            - pypi:
                package: pydantic~=2.10  
        - task_key: inf_prep
          depends_on:
            - task_key: checkpoint
            - task_key: student_selection
          spark_python_task:
            python_file: src/edvise/scripts/pdp_inf_prep.py
            source: GIT
            parameters:
              - --silver_volume_path
              - "/Volumes/{{job.parameters.DB_workspace}}/{{job.parameters.databricks_institution_name}}_silver/silver_volume"
              - --config_file_path 
              - "/Volumes/{{job.parameters.DB_workspace}}/{{job.parameters.databricks_institution_name}}_bronze/bronze_volume/training_inputs/{{job.parameters.config_file_name}}"
          job_cluster_key: edvise-pdp-inference-pipeline-cluster
          libraries:
            - pypi:
                package: pandas==2.2.3
            - pypi:
                package: numpy==1.26.4
            - pypi:
                package: faker==33.1.0
            - pypi:
                package: google-cloud-storage==2.19.0
            - pypi:
                package: missingno==0.5.2
            - pypi:
                package: tomli==2.2.1
            - pypi:
                package: tomlkit==0.13.3
            - pypi:
                package: pandera==0.23.0
            - pypi:
                package: pydantic~=2.10      
        - task_key: model_type
          depends_on:
            - task_key: inf_prep
          condition_task:
            op: EQUAL_TO
            left: "{{tasks.inf_prep.values.model_type}}"
            right: h2o
        - task_key: inference_sklearn
          depends_on:
            - task_key: model_type
              outcome: "false"
          spark_python_task:
            python_file: src/edvise/scripts/inference_sklearn.py
            source: GIT
            parameters:
              - --silver_table_path
              - "{{job.parameters.DB_workspace}}.{{job.parameters.databricks_institution_name}}_silver"
              - --silver_volume_path
              - "/Volumes/{{job.parameters.DB_workspace}}/{{job.parameters.databricks_institution_name}}_silver/silver_volume"
              - --config_file_path 
              - "/Volumes/{{job.parameters.DB_workspace}}/{{job.parameters.databricks_institution_name}}_bronze/bronze_volume/training_inputs/{{job.parameters.config_file_name}}"
              - --db_run_id
              - "{{job.parameters.db_run_id}}"
              - --ds_run_as
              - "{{job.parameters.ds_run_as}}"
              - --DB_workspace
              - "{{job.parameters.DB_workspace}}"
              - --databricks_institution_name
              - "{{job.parameters.databricks_institution_name}}"
              - --gold_table_path
              - "{{job.parameters.DB_workspace}}/{{job.parameters.databricks_institution_name}}_gold"
              - --datakind_notification_email
              - "{{job.parameters.datakind_notification_email}}"
              - --DK_CC_EMAIL
              - "{{job.parameters.DK_CC_EMAIL}}"
              - --job_root_dir  
              - "/Volumes/{{job.parameters.DB_workspace}}/{{job.parameters.databricks_institution_name}}_gold/gold_volume/inference_jobs/{{job.parameters.db_run_id}}"
          job_cluster_key: edvise-sklearn-inference-cluster
          libraries:
            - pypi:
                package: numpy==1.24.4 
            - pypi:
                package: pandas==1.5.3
            - pypi:
                package: faker==33.1.0
            - pypi:
                package: google-cloud-storage==2.19.0
            - pypi:
                package: scikit-learn==1.3.0
            - pypi:
                package: missingno==0.5.2
            - pypi:
                package: tomli==2.2.1
            - pypi:
                package: tomlkit==0.13.3
            - pypi:
                package: pandera==0.16.1
            - pypi:
                package: pydantic<2.0.0
            - pypi:
                package: shap~=0.46.0
            - pypi:
                package: mlflow~=2.22
            - pypi:
                package: h2o==3.46.0.7
            - pypi:
                package: cloudpickle==2.2.1
            - pypi:
                package: category-encoders==2.6.3
            - pypi:
                package: databricks-automl-runtime==0.2.21
            - pypi:
                package: holidays==0.45
            - pypi:
                package: lz4==4.3.2
            - pypi:
                package: psutil==5.9.0
        - task_key: inference_h2o
          depends_on:
            - task_key: model_type
              outcome: "true"
          spark_python_task:
            python_file: src/edvise/scripts/inference_h2o.py
            source: GIT
            parameters:
              - --silver_volume_path
              - "/Volumes/{{job.parameters.DB_workspace}}/{{job.parameters.databricks_institution_name}}_silver/silver_volume"
              - --config_file_path 
              - "/Volumes/{{job.parameters.DB_workspace}}/{{job.parameters.databricks_institution_name}}_bronze/bronze_volume/training_inputs/{{job.parameters.config_file_name}}"
              - --db_run_id
              - "{{job.parameters.db_run_id}}"
              - --ds_run_as
              - "{{job.parameters.ds_run_as}}"
              - --DB_workspace
              - "{{job.parameters.DB_workspace}}"
              - --databricks_institution_name
              - "{{job.parameters.databricks_institution_name}}"
              - --datakind_notification_email
              - "{{job.parameters.datakind_notification_email}}"
              - --DK_CC_EMAIL
              - "{{job.parameters.DK_CC_EMAIL}}"
              - --job_root_dir  
              - "/Volumes/{{job.parameters.DB_workspace}}/{{job.parameters.databricks_institution_name}}_gold/gold_volume/inference_jobs/{{job.parameters.db_run_id}}"
          job_cluster_key: edvise-pdp-inference-pipeline-cluster
          libraries:
            - pypi:
                package: numpy==1.26.4
            - pypi:
                package: pandas==2.2.3
            - pypi:
                package: faker==33.1.0
            - pypi:
                package: google-cloud-storage==2.19.0
            - pypi:
                package: missingno==0.5.2
            - pypi:
                package: tomli==2.2.1
            - pypi:
                package: tomlkit==0.13.3
            - pypi:
                package: pandera==0.23.0
            - pypi:
                package: pydantic~=2.10
            - pypi:
                package: h2o==3.46.0.7
            - pypi:
                package: shap~=0.46.0
            - pypi:
                package: mlflow~=2.22
            - pypi:
                package: scikit-learn==1.5.2
        - task_key: output_publish
          depends_on:
            - task_key: inference_sklearn
            - task_key: inference_h2o
          run_if: AT_LEAST_ONE_SUCCESS
          spark_python_task: 
            python_file: src/edvise/scripts/pdp_inference_output_publish.py
            source: GIT
            parameters:
              - --DB_workspace
              - "{{job.parameters.DB_workspace}}"
              - --databricks_institution_name
              - "{{job.parameters.databricks_institution_name}}"
              - --db_run_id
              - "{{job.parameters.db_run_id}}"
              - --gcp_bucket_name
              - "{{job.parameters.gcp_bucket_name}}"
              - --datakind_notification_email
              - "{{job.parameters.datakind_notification_email}}"
          job_cluster_key: edvise-pdp-inference-pipeline-cluster
          libraries:
            - pypi:
                package: pandas==2.2.3
            - pypi:
                package: google-cloud-storage==2.19.0
      job_clusters:
        - job_cluster_key: edvise-pdp-inference-pipeline-cluster
          new_cluster:
            cluster_name: ""
            spark_version: 15.4.x-cpu-ml-scala2.12
            spark_conf:
              spark.master: local[*, 4]
              spark.databricks.cluster.profile: singleNode
            gcp_attributes:
              use_preemptible_executors: false
              availability: ON_DEMAND_GCP
              zone_id: HA
            node_type_id: n2-standard-16
            custom_tags:
              ResourceClass: SingleNode
              x-databricks-nextgen-cluster: "true"
            enable_elastic_disk: true
            data_security_mode: SINGLE_USER
            runtime_engine: STANDARD
            num_workers: 0
        - job_cluster_key: edvise-sklearn-inference-cluster
          new_cluster:
            cluster_name: ""
            spark_version: 15.4.x-cpu-ml-scala2.12
            spark_conf:
              spark.master: local[*, 4]
              spark.databricks.cluster.profile: singleNode
            gcp_attributes:
              use_preemptible_executors: false
              availability: ON_DEMAND_GCP
              zone_id: HA
            node_type_id: n2-standard-16
            custom_tags:
              ResourceClass: SingleNode
              x-databricks-nextgen-cluster: "true"
            enable_elastic_disk: true
            data_security_mode: SINGLE_USER
            runtime_engine: STANDARD
            num_workers: 0
      git_source:
        git_url: https://github.com/datakind/edvise
        git_provider: gitHub
        # git_tag: v0.1.2
        git_branch: develop
      tags:
        dev: kayla_wilding
      queue:
        enabled: true
      parameters:
        - name: cohort_file_name
          default: AO1600pdp_AO1600_AR_DEIDENTIFIED_STUDYID_20250819030854.csv
        - name: course_file_name
          default: AO1600pdp_AO1600_COURSE_LEVEL_AR_DEIDENTIFIED_STUDYID_20250819030854.csv
        - name: databricks_institution_name
          default: ${var.databricks_institution_name}
        - name: db_run_id
          default: "{{job.run_id}}"
        - name: DB_workspace
          default: ${var.DB_workspace}
        - name: DK_CC_EMAIL
          default: ${var.datakind_notification_email}
        - name: config_file_name
          default: ${var.config_file_name}
        - name: ds_run_as
          default: ${var.ds_run_as}
        - name: gcp_bucket_name
          default: databricks-2052166062819251-unitycatalog
        - name: datakind_notification_email
          default: ${var.datakind_notification_email}
        - name: service_account_executer
          default: ${var.service_account_executer}
        - name: model_type
          default: sklearn
        - name: job_type
          default: inference
      permissions:
        - group_name: ${var.datakind_group_to_manage_workflow} 
          level: CAN_MANAGE
        - user_name: ${var.service_account_executer}
          level: CAN_MANAGE_RUN