name: weekly-develop-integration

on: 
  # weekly health monitor for develop branch 
  schedule:
    - cron: "0 3 * * 1"   # Mondays 03:00 UTC – adjust as needed
  workflow_dispatch: {}    # allow manual runs

concurrency:
  group: weekly-develop-integration
  cancel-in-progress: false

jobs:
  dev-train-infer-develop-health-check:
    name: Train + Inference (Dev) – develop-health-check
    runs-on: ubuntu-latest
    timeout-minutes: 120

    env:
      CONFIG_FILE: integration_config.toml
      DB_RUN_ID_PREFIX: weekly-${{ github.run_id }}

    steps:
      - uses: actions/checkout@v4
        with:
          ref: develop
          fetch-depth: 0

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11.11"

      - name: Install uv and Databricks CLI
        run: |
          python -m pip install --upgrade pip pipx
          pipx ensurepath
          pipx install uv
          pipx install databricks
        shell: bash

      - name: Install project dependencies (uv)
        run: |
          uv venv
          uv pip install .
        shell: bash

      - name: Authenticate Databricks (Dev)
        env:
          DATABRICKS_HOST: ${{ secrets.DATABRICKS_DEV_HOST }}
          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_DEV_TOKEN }}
        run: |
          databricks auth login --host "$DATABRICKS_HOST" --token "$DATABRICKS_TOKEN"
          databricks auth validate

      - name: Deploy bundle to Dev (pin to develop HEAD)
        env:
          DB_WORKSPACE: ${{ secrets.DEV_DB_WORKSPACE }}
          SA_EXECUTER: ${{ secrets.DEV_SERVICE_ACCOUNT_EXECUTER }}
          SP_RUN_AS: ${{ secrets.DEV_SERVICE_PRINCIPAL_RUN_AS }}
          GROUP_TO_MANAGE: ${{ secrets.DEV_GROUP_TO_MANAGE }}
          PIPELINE_SA_EMAIL: ${{ secrets.DEV_PIPELINE_SA_EMAIL }}
          END_USER_EMAIL: ${{ secrets.DEV_END_USER_EMAIL }}
          DATAKIND_EMAIL: ${{ secrets.DEV_DATAKIND_EMAIL }}
          SCHEMAS_PATH: ${{ secrets.DEV_CUSTOM_SCHEMAS_PATH }}
        run: |
          databricks bundle deploy \
            --target=dev \
            --var="DB_workspace=$DB_WORKSPACE" \
            --var="service_account_executer=$SA_EXECUTER" \
            --var="service_principal_run_as=$SP_RUN_AS" \
            --var="datakind_group_to_manage_workflow=$GROUP_TO_MANAGE" \
            --var="pipeline_sa_email=$PIPELINE_SA_EMAIL" \
            --var="end_user_notification_email=$END_USER_EMAIL" \
            --var="datakind_notification_email=$DATAKIND_EMAIL" \
            --var="custom_schemas_path=$SCHEMAS_PATH"
            # If you template git_source, add: --var="git_ref=${{ github.sha }}"

      - name: TRAIN (Dev – develop-health-check)
        run: |
          databricks bundle run github_sourced_pdp_training_pipeline --target dev \
            --params-json '{
              "config_file_name": "'"$CONFIG_FILE"'",
              "job_type": "training",
              "db_run_id": "'"$DB_RUN_ID_PREFIX"'-train"
            }'

      - name: INFER (Dev – develop-health-check)
        run: |
          databricks bundle run edvise_github_sourced_pdp_inference_pipeline --target dev \
            --params-json '{
              "config_file_name": "'"$CONFIG_FILE"'",
              "job_type": "inference",
              "db_run_id": "'"$DB_RUN_ID_PREFIX"'-infer"
            }'
