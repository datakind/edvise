{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Connect to SFTP and scan the receive folder for files.\n",
    "#2. Upsert unseen files into `ingestion_manifest` with status=NEW.\n",
    "#3. Download and stage NEW + unqueued files locally and upsert them into `pending_ingest_queue`.\n",
    "\n",
    "#Recent refactor:\n",
    "#- SFTP helpers moved to `helper.py` (`connect_sftp`, `list_receive_files`, `download_sftp_atomic`).\n",
    "#- `list_receive_files` now takes `source_system` explicitly (no hidden notebook globals).\n",
    "\n",
    "#Constraints:\n",
    "# - SFTP connection required\n",
    "# - NO API calls\n",
    "# - Stages files locally (TMP_DIR) + writes to Delta tables only\n",
    "\n",
    "#Inputs:\n",
    "#- SFTP folder: `./receive`\n",
    "\n",
    "#Outputs:\n",
   "#- `staging_sst_01.default.ingestion_manifest`\n",
   "#- `staging_sst_01.default.pending_ingest_queue`\n",
   "#- Staged files written to: `/tmp/pdp_sftp_stage`\n"
  ]
 },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbd7694b-4b30-41bf-9371-259479726010",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install paramiko python-box pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9ae88af-ade1-4df0-86a0-34d6d492383a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5888f9b8-bda7-4586-9f9f-ed1243d878de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import paramiko\n",
    "from box import Box\n",
    "from datetime import datetime, timezone\n",
    "from databricks.connect import DatabricksSession\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "from helper import CustomLogger, connect_sftp, list_receive_files, download_sftp_atomic\n",
    "\n",
    "try:\n",
    "    dbutils  # noqa: F821\n",
    "except NameError:\n",
    "    from unittest.mock import MagicMock\n",
    "\n",
    "    dbutils = MagicMock()\n",
    "spark = DatabricksSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61b348b8-aa62-4b5a-9442-d48d52e1a862",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logger = CustomLogger()\n",
    "\n",
    "# Config + Secrets (kept consistent with existing pipeline)\n",
    "with open(\"gcp_config.yaml\", \"rb\") as f:\n",
    "    cfg = Box(yaml.safe_load(f))\n",
    "\n",
    "asset_scope = cfg.institution.secure_assets[\"scope\"]\n",
    "\n",
    "host = dbutils.secrets.get(scope=asset_scope, key=cfg.pdp.secret[\"keys\"][\"host\"])\n",
    "user = dbutils.secrets.get(scope=asset_scope, key=cfg.pdp.secret[\"keys\"][\"user\"])\n",
    "password = dbutils.secrets.get(\n",
    "    scope=asset_scope, key=cfg.pdp.secret[\"keys\"][\"password\"]\n",
    ")\n",
    "\n",
    "remote_folder = \"./receive\"\n",
    "source_system = \"NSC\"\n",
    "\n",
    "CATALOG = \"staging_sst_01\"\n",
    "DEFAULT_SCHEMA = \"default\"\n",
    "MANIFEST_TABLE = f\"{CATALOG}.{DEFAULT_SCHEMA}.ingestion_manifest\"\n",
    "QUEUE_TABLE = f\"{CATALOG}.{DEFAULT_SCHEMA}.pending_ingest_queue\"\n",
    "\n",
   "TMP_DIR = \"/tmp/pdp_sftp_stage\"\n",
    "\n",
    "logger.info(\"SFTP secured assets loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8533c9ea-059a-46cf-a847-c235c35968d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# moved to helper.py: connect_sftp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e26601a-d0fd-4dad-826e-534b03920dbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def ensure_tables():\n",
    "    \"\"\"\n",
    "    Create required delta tables if missing.\n",
    "    - ingestion_manifest: includes file_fingerprint for idempotency\n",
    "    - pending_ingest_queue: holds local tmp path so downstream doesn't connect to SFTP again\n",
    "    \"\"\"\n",
    "    spark.sql(\n",
    "        f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {MANIFEST_TABLE} (\n",
    "          file_fingerprint STRING,\n",
    "          source_system STRING,\n",
    "          sftp_path STRING,\n",
    "          file_name STRING,\n",
    "          file_size BIGINT,\n",
    "          file_modified_time TIMESTAMP,\n",
    "          ingested_at TIMESTAMP,\n",
    "          processed_at TIMESTAMP,\n",
    "          status STRING,\n",
    "          error_message STRING\n",
    "        )\n",
    "        USING DELTA\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    spark.sql(\n",
    "        f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {QUEUE_TABLE} (\n",
    "          file_fingerprint STRING,\n",
    "          source_system STRING,\n",
    "          sftp_path STRING,\n",
    "          file_name STRING,\n",
    "          file_size BIGINT,\n",
    "          file_modified_time TIMESTAMP,\n",
    "          local_tmp_path STRING,\n",
    "          queued_at TIMESTAMP\n",
    "        )\n",
    "        USING DELTA\n",
    "        \"\"\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88771dfe-1ac5-47bb-9b3d-5d74031cc8d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# moved to helper.py: list_receive_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5ea3757-0f48-44d1-9050-e4fa07e1f57b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def build_listing_df(file_rows):\n",
    "    schema = T.StructType(\n",
    "        [\n",
    "            T.StructField(\"source_system\", T.StringType(), False),\n",
    "            T.StructField(\"sftp_path\", T.StringType(), False),\n",
    "            T.StructField(\"file_name\", T.StringType(), False),\n",
    "            T.StructField(\"file_size\", T.LongType(), True),\n",
    "            T.StructField(\"file_modified_time\", T.TimestampType(), True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    df = spark.createDataFrame(file_rows, schema=schema)\n",
    "\n",
    "    # Stable fingerprint from metadata (file version identity)\n",
    "    # Note: cast mtime to string in a consistent format to avoid subtle timestamp formatting diffs.\n",
    "    df = df.withColumn(\n",
    "        \"file_fingerprint\",\n",
    "        F.sha2(\n",
    "            F.concat_ws(\n",
    "                \"||\",\n",
    "                F.col(\"source_system\"),\n",
    "                F.col(\"sftp_path\"),\n",
    "                F.col(\"file_name\"),\n",
    "                F.coalesce(F.col(\"file_size\").cast(\"string\"), F.lit(\"\")),\n",
    "                F.coalesce(\n",
    "                    F.date_format(\n",
    "                        F.col(\"file_modified_time\"), \"yyyy-MM-dd'T'HH:mm:ss.SSSXXX\"\n",
    "                    ),\n",
    "                    F.lit(\"\"),\n",
    "                ),\n",
    "            ),\n",
    "            256,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "397c00f3-4486-49c4-902d-b63d6c31b9ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def upsert_new_to_manifest(df_listing):\n",
    "    \"\"\"\n",
    "    Insert NEW rows for unseen fingerprints only.\n",
    "    \"\"\"\n",
    "    df_manifest_insert = (\n",
    "        df_listing.select(\n",
    "            \"file_fingerprint\",\n",
    "            \"source_system\",\n",
    "            \"sftp_path\",\n",
    "            \"file_name\",\n",
    "            \"file_size\",\n",
    "            \"file_modified_time\",\n",
    "        )\n",
    "        .withColumn(\"ingested_at\", F.lit(None).cast(\"timestamp\"))\n",
    "        .withColumn(\"processed_at\", F.lit(None).cast(\"timestamp\"))\n",
    "        .withColumn(\"status\", F.lit(\"NEW\"))\n",
    "        .withColumn(\"error_message\", F.lit(None).cast(\"string\"))\n",
    "    )\n",
    "\n",
    "    df_manifest_insert.createOrReplaceTempView(\"incoming_manifest_rows\")\n",
    "\n",
    "    spark.sql(\n",
    "        f\"\"\"\n",
    "        MERGE INTO {MANIFEST_TABLE} AS t\n",
    "        USING incoming_manifest_rows AS s\n",
    "        ON t.file_fingerprint = s.file_fingerprint\n",
    "        WHEN NOT MATCHED THEN INSERT *\n",
    "        \"\"\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40774249-08a4-4063-9e33-b35f11423b9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_files_to_queue(df_listing):\n",
    "    \"\"\"\n",
    "    Return files that should be queued for downstream processing.\n",
    "\n",
    "    Criteria:\n",
    "      - present in current SFTP listing (df_listing)\n",
    "      - exist in manifest with status = 'NEW'\n",
    "      - NOT already present in pending_ingest_queue\n",
    "    \"\"\"\n",
    "    manifest_new = (\n",
    "        spark.table(MANIFEST_TABLE)\n",
    "        .select(\"file_fingerprint\", \"status\")\n",
    "        .where(F.col(\"status\") == F.lit(\"NEW\"))\n",
    "        .select(\"file_fingerprint\")\n",
    "    )\n",
    "\n",
    "    already_queued = spark.table(QUEUE_TABLE).select(\"file_fingerprint\").distinct()\n",
    "\n",
    "    # Only queue files that are:\n",
    "    #   in current listing AND in manifest NEW AND not in queue\n",
    "    to_queue = df_listing.join(manifest_new, on=\"file_fingerprint\", how=\"inner\").join(\n",
    "        already_queued, on=\"file_fingerprint\", how=\"left_anti\"\n",
    "    )\n",
    "    return to_queue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "499787be-ca97-4f30-9140-1fcf57d620ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# moved to helper.py: _hash_file, _remote_hash, download_sftp_atomic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53f05063-ec80-4a41-9611-641331b7f462",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def download_new_files_and_queue(sftp: paramiko.SFTPClient, df_new):\n",
    "    \"\"\"\n",
    "    Download each new file to /tmp and upsert into pending_ingest_queue.\n",
    "    \"\"\"\n",
    "    os.makedirs(TMP_DIR, exist_ok=True)\n",
    "\n",
    "    # Collect is OK if you expect modest number of files. If you expect thousands, we can paginate and stream.\n",
    "    rows = df_new.select(\n",
    "        \"file_fingerprint\",\n",
    "        \"source_system\",\n",
    "        \"sftp_path\",\n",
    "        \"file_name\",\n",
    "        \"file_size\",\n",
    "        \"file_modified_time\",\n",
    "    ).collect()\n",
    "\n",
    "    queued = []\n",
    "    for r in rows:\n",
    "        fp = r[\"file_fingerprint\"]\n",
    "        sftp_path = r[\"sftp_path\"]\n",
    "        file_name = r[\"file_name\"]\n",
    "\n",
    "        remote_path = f\"{sftp_path.rstrip('/')}/{file_name}\"\n",
    "        local_path = os.path.abspath(os.path.join(TMP_DIR, f\"{fp}__{file_name}\"))\n",
    "\n",
    "        # If local already exists (e.g., rerun), skip re-download\n",
    "        if not os.path.exists(local_path):\n",
    "            print(f\"Downloading new file from SFTP: {remote_path} -> {local_path}\")\n",
    "            logger.info(\n",
    "                f\"Downloading new file from SFTP: {remote_path} -> {local_path}\"\n",
    "            )\n",
    "            # sftp.get(remote_path, local_path)\n",
    "            download_sftp_atomic(sftp, remote_path, local_path, chunk=150)\n",
    "        else:\n",
    "            print(f\"Skipping download, file already exists: {local_path}\")\n",
    "            logger.info(f\"Local file already staged, skipping download: {local_path}\")\n",
    "\n",
    "        queued.append(\n",
    "            {\n",
    "                \"file_fingerprint\": fp,\n",
    "                \"source_system\": r[\"source_system\"],\n",
    "                \"sftp_path\": sftp_path,\n",
    "                \"file_name\": file_name,\n",
    "                \"file_size\": r[\"file_size\"],\n",
    "                \"file_modified_time\": r[\"file_modified_time\"],\n",
    "                \"local_tmp_path\": local_path,\n",
    "                \"queued_at\": datetime.now(timezone.utc),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    if not queued:\n",
    "        return 0\n",
    "\n",
    "    qschema = T.StructType(\n",
    "        [\n",
    "            T.StructField(\"file_fingerprint\", T.StringType(), False),\n",
    "            T.StructField(\"source_system\", T.StringType(), False),\n",
    "            T.StructField(\"sftp_path\", T.StringType(), False),\n",
    "            T.StructField(\"file_name\", T.StringType(), False),\n",
    "            T.StructField(\"file_size\", T.LongType(), True),\n",
    "            T.StructField(\"file_modified_time\", T.TimestampType(), True),\n",
    "            T.StructField(\"local_tmp_path\", T.StringType(), False),\n",
    "            T.StructField(\"queued_at\", T.TimestampType(), False),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    df_queue = spark.createDataFrame(queued, schema=qschema)\n",
    "    df_queue.createOrReplaceTempView(\"incoming_queue_rows\")\n",
    "\n",
    "    # Upsert into queue (idempotent by fingerprint)\n",
    "\n",
    "    spark.sql(\n",
    "        f\"\"\"\n",
    "        MERGE INTO {QUEUE_TABLE} AS t\n",
    "        USING incoming_queue_rows AS s\n",
    "        ON t.file_fingerprint = s.file_fingerprint\n",
    "        WHEN MATCHED THEN UPDATE SET\n",
    "        t.local_tmp_path = s.local_tmp_path,\n",
    "        t.queued_at = s.queued_at\n",
    "        WHEN NOT MATCHED THEN INSERT *\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    return len(queued)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80968f66-5082-49ca-b03f-b3a1ef0bb908",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "transport = None\n",
    "sftp = None\n",
    "\n",
    "try:\n",
    "    ensure_tables()\n",
    "\n",
    "    transport, sftp = connect_sftp(host, user, password)\n",
    "    logger.info(f\"Connected to SFTP host={host} and scanning folder={remote_folder}\")\n",
    "\n",
    "    file_rows = list_receive_files(sftp, remote_folder, source_system)\n",
    "    if not file_rows:\n",
    "        logger.info(f\"No files found in SFTP folder: {remote_folder}. Exiting (no-op).\")\n",
    "        dbutils.notebook.exit(\"NO_FILES\")\n",
    "\n",
    "    df_listing = build_listing_df(file_rows)\n",
    "\n",
    "    # 1) Ensure everything on SFTP is at least represented in manifest as NEW\n",
    "    upsert_new_to_manifest(df_listing)\n",
    "\n",
    "    # 2) Queue anything that is still NEW and not already queued\n",
    "    df_to_queue = get_files_to_queue(df_listing)\n",
    "\n",
    "    to_queue_count = df_to_queue.count()\n",
    "    if to_queue_count == 0:\n",
    "        logger.info(\n",
    "            \"No files to queue: either nothing is NEW, or NEW files are already queued. Exiting (no-op).\"\n",
    "        )\n",
    "        dbutils.notebook.exit(\"QUEUED_FILES=0\")\n",
    "\n",
    "    logger.info(\n",
    "        f\"Queuing {to_queue_count} NEW-unqueued file(s) to {QUEUE_TABLE} and staging locally.\"\n",
    "    )\n",
    "    queued_count = download_new_files_and_queue(sftp, df_to_queue)\n",
    "\n",
    "    logger.info(\n",
    "        f\"Queued {queued_count} file(s) for downstream processing in {QUEUE_TABLE}.\"\n",
    "    )\n",
    "    dbutils.notebook.exit(f\"QUEUED_FILES={queued_count}\")\n",
    "\n",
    "finally:\n",
    "    try:\n",
    "        if sftp is not None:\n",
    "            sftp.close()\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        if transport is not None:\n",
    "            transport.close()\n",
    "    except Exception:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80a87ce4-8f44-449e-bef7-f40a73e60bf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_sftp_receive_scan",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
