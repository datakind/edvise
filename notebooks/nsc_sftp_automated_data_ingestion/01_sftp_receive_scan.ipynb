{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbd7694b-4b30-41bf-9371-259479726010",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install paramiko python-box pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9ae88af-ade1-4df0-86a0-34d6d492383a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5888f9b8-bda7-4586-9f9f-ed1243d878de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import stat\n",
    "import yaml\n",
    "import paramiko\n",
    "from box import Box\n",
    "from datetime import datetime, timezone\n",
    "import hashlib\n",
    "import shlex\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "from helper import CustomLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61b348b8-aa62-4b5a-9442-d48d52e1a862",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logger = CustomLogger()\n",
    "\n",
    "# Config + Secrets (kept consistent with existing pipeline)\n",
    "with open(\"gcp_config.yaml\", \"rb\") as f:\n",
    "    cfg = Box(yaml.safe_load(f))\n",
    "\n",
    "asset_scope = cfg.institution.secure_assets[\"scope\"]\n",
    "\n",
    "host = dbutils.secrets.get(scope=asset_scope, key=cfg.pdp.secret[\"keys\"][\"host\"])\n",
    "user = dbutils.secrets.get(scope=asset_scope, key=cfg.pdp.secret[\"keys\"][\"user\"])\n",
    "password = dbutils.secrets.get(scope=asset_scope, key=cfg.pdp.secret[\"keys\"][\"password\"])\n",
    "\n",
    "remote_folder = \"./receive\"\n",
    "source_system = \"NSC\"\n",
    "\n",
    "CATALOG = \"staging_sst_01\"\n",
    "DEFAULT_SCHEMA = \"default\"\n",
    "MANIFEST_TABLE = f\"{CATALOG}.{DEFAULT_SCHEMA}.ingestion_manifest\"\n",
    "QUEUE_TABLE = f\"{CATALOG}.{DEFAULT_SCHEMA}.pending_ingest_queue\"\n",
    "\n",
    "TMP_DIR = \"./tmp/pdp_sftp_stage\"\n",
    "\n",
    "logger.info(\"SFTP secured assets loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8533c9ea-059a-46cf-a847-c235c35968d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def connect_sftp(host: str, username: str, password: str, port: int = 22):\n",
    "    \"\"\"\n",
    "    Return (transport, sftp_client). Caller must close both.\n",
    "    \"\"\"\n",
    "    transport = paramiko.Transport((host, port))\n",
    "    transport.connect(username=username, password=password)\n",
    "    sftp = paramiko.SFTPClient.from_transport(transport)\n",
    "    print(f\"Connected successfully to {host}\")\n",
    "    return transport, sftp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e26601a-d0fd-4dad-826e-534b03920dbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def ensure_tables():\n",
    "    \"\"\"\n",
    "    Create required delta tables if missing.\n",
    "    - ingestion_manifest: includes file_fingerprint for idempotency\n",
    "    - pending_ingest_queue: holds local tmp path so downstream doesn't connect to SFTP again\n",
    "    \"\"\"\n",
    "    spark.sql(\n",
    "        f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {MANIFEST_TABLE} (\n",
    "          file_fingerprint STRING,\n",
    "          source_system STRING,\n",
    "          sftp_path STRING,\n",
    "          file_name STRING,\n",
    "          file_size BIGINT,\n",
    "          file_modified_time TIMESTAMP,\n",
    "          ingested_at TIMESTAMP,\n",
    "          processed_at TIMESTAMP,\n",
    "          status STRING,\n",
    "          error_message STRING\n",
    "        )\n",
    "        USING DELTA\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    spark.sql(\n",
    "        f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {QUEUE_TABLE} (\n",
    "          file_fingerprint STRING,\n",
    "          source_system STRING,\n",
    "          sftp_path STRING,\n",
    "          file_name STRING,\n",
    "          file_size BIGINT,\n",
    "          file_modified_time TIMESTAMP,\n",
    "          local_tmp_path STRING,\n",
    "          queued_at TIMESTAMP\n",
    "        )\n",
    "        USING DELTA\n",
    "        \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88771dfe-1ac5-47bb-9b3d-5d74031cc8d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def list_receive_files(sftp: paramiko.SFTPClient, remote_dir: str):\n",
    "    \"\"\"\n",
    "    List non-directory files in remote_dir with metadata.\n",
    "    Returns list[dict] with keys: source_system, sftp_path, file_name, file_size, file_modified_time\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for attr in sftp.listdir_attr(remote_dir):\n",
    "        if stat.S_ISDIR(attr.st_mode):\n",
    "            continue\n",
    "\n",
    "        file_name = attr.filename\n",
    "        file_size = int(attr.st_size) if attr.st_size is not None else None\n",
    "        mtime = datetime.fromtimestamp(int(attr.st_mtime), tz=timezone.utc) if attr.st_mtime else None\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"source_system\": source_system,\n",
    "                \"sftp_path\": remote_dir,\n",
    "                \"file_name\": file_name,\n",
    "                \"file_size\": file_size,\n",
    "                \"file_modified_time\": mtime,\n",
    "            }\n",
    "        )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5ea3757-0f48-44d1-9050-e4fa07e1f57b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def build_listing_df(file_rows):\n",
    "    schema = T.StructType(\n",
    "        [\n",
    "            T.StructField(\"source_system\", T.StringType(), False),\n",
    "            T.StructField(\"sftp_path\", T.StringType(), False),\n",
    "            T.StructField(\"file_name\", T.StringType(), False),\n",
    "            T.StructField(\"file_size\", T.LongType(), True),\n",
    "            T.StructField(\"file_modified_time\", T.TimestampType(), True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    df = spark.createDataFrame(file_rows, schema=schema)\n",
    "\n",
    "    # Stable fingerprint from metadata (file version identity)\n",
    "    # Note: cast mtime to string in a consistent format to avoid subtle timestamp formatting diffs.\n",
    "    df = df.withColumn(\n",
    "        \"file_fingerprint\",\n",
    "        F.sha2(\n",
    "            F.concat_ws(\n",
    "                \"||\",\n",
    "                F.col(\"source_system\"),\n",
    "                F.col(\"sftp_path\"),\n",
    "                F.col(\"file_name\"),\n",
    "                F.coalesce(F.col(\"file_size\").cast(\"string\"), F.lit(\"\")),\n",
    "                F.coalesce(F.date_format(F.col(\"file_modified_time\"), \"yyyy-MM-dd'T'HH:mm:ss.SSSXXX\"), F.lit(\"\")),\n",
    "            ),\n",
    "            256,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "397c00f3-4486-49c4-902d-b63d6c31b9ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def upsert_new_to_manifest(df_listing):\n",
    "    \"\"\"\n",
    "    Insert NEW rows for unseen fingerprints only.\n",
    "    \"\"\"\n",
    "    df_manifest_insert = (\n",
    "        df_listing.select(\n",
    "            \"file_fingerprint\",\n",
    "            \"source_system\",\n",
    "            \"sftp_path\",\n",
    "            \"file_name\",\n",
    "            \"file_size\",\n",
    "            \"file_modified_time\",\n",
    "        )\n",
    "        .withColumn(\"ingested_at\", F.lit(None).cast(\"timestamp\"))\n",
    "        .withColumn(\"processed_at\", F.lit(None).cast(\"timestamp\"))\n",
    "        .withColumn(\"status\", F.lit(\"NEW\"))\n",
    "        .withColumn(\"error_message\", F.lit(None).cast(\"string\"))\n",
    "    )\n",
    "\n",
    "    df_manifest_insert.createOrReplaceTempView(\"incoming_manifest_rows\")\n",
    "\n",
    "    spark.sql(\n",
    "        f\"\"\"\n",
    "        MERGE INTO {MANIFEST_TABLE} AS t\n",
    "        USING incoming_manifest_rows AS s\n",
    "        ON t.file_fingerprint = s.file_fingerprint\n",
    "        WHEN NOT MATCHED THEN INSERT *\n",
    "        \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40774249-08a4-4063-9e33-b35f11423b9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_files_to_queue(df_listing):\n",
    "    \"\"\"\n",
    "    Return files that should be queued for downstream processing.\n",
    "\n",
    "    Criteria:\n",
    "      - present in current SFTP listing (df_listing)\n",
    "      - exist in manifest with status = 'NEW'\n",
    "      - NOT already present in pending_ingest_queue\n",
    "    \"\"\"\n",
    "    manifest_new = (\n",
    "        spark.table(MANIFEST_TABLE)\n",
    "        .select(\"file_fingerprint\", \"status\")\n",
    "        .where(F.col(\"status\") == F.lit(\"NEW\"))\n",
    "        .select(\"file_fingerprint\")\n",
    "    )\n",
    "\n",
    "    already_queued = spark.table(QUEUE_TABLE).select(\"file_fingerprint\").distinct()\n",
    "\n",
    "    # Only queue files that are:\n",
    "    #   in current listing AND in manifest NEW AND not in queue\n",
    "    to_queue = (\n",
    "        df_listing.join(manifest_new, on=\"file_fingerprint\", how=\"inner\")\n",
    "                 .join(already_queued, on=\"file_fingerprint\", how=\"left_anti\")\n",
    "    )\n",
    "    return to_queue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "499787be-ca97-4f30-9140-1fcf57d620ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def _hash_file(path, algo=\"sha256\", chunk_size=8 * 1024 * 1024):\n",
    "    h = hashlib.new(algo)\n",
    "    with open(path, \"rb\") as f:\n",
    "        while True:\n",
    "            b = f.read(chunk_size)\n",
    "            if not b:\n",
    "                break\n",
    "            h.update(b)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def _remote_hash(ssh, remote_path, algo=\"sha256\"):\n",
    "    cmd = None\n",
    "    if algo.lower() == \"sha256\":\n",
    "        cmd = f\"sha256sum -- {shlex.quote(remote_path)}\"\n",
    "    elif algo.lower() == \"md5\":\n",
    "        cmd = f\"md5sum -- {shlex.quote(remote_path)}\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        _, stdout, stderr = ssh.exec_command(cmd, timeout=300)\n",
    "        out = stdout.read().decode(\"utf-8\", \"replace\").strip()\n",
    "        err = stderr.read().decode(\"utf-8\", \"replace\").strip()\n",
    "        if err:\n",
    "            return None\n",
    "        # Format: \"<hash>  <filename>\"\n",
    "        return out.split()[0]\n",
    "    except Exception:\n",
    "        return None\n",
    "    \n",
    "def download_sftp_atomic(\n",
    "    sftp,\n",
    "    remote_path,\n",
    "    local_path,\n",
    "    *,\n",
    "    chunk: int = 150,\n",
    "    verify=\"size\", # \"size\" | \"sha256\" | \"md5\" | None\n",
    "    ssh_for_remote_hash=None, # paramiko.SSHClient if you want remote hash verify\n",
    "    progress=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Atomic + resumable SFTP download that never trims data in situ.\n",
    "    Writes to local_path + '.part' and moves into place after verification.\n",
    "    \"\"\"\n",
    "    remote_size = sftp.stat(remote_path).st_size\n",
    "    tmp_path = f\"{local_path}.part\"\n",
    "    chunk_size = chunk * 1024 * 1024\n",
    "    offset = 0\n",
    "    if os.path.exists(tmp_path):\n",
    "        part_size = os.path.getsize(tmp_path)\n",
    "        # If local .part is larger than remote, start fresh.\n",
    "        if part_size <= remote_size:\n",
    "            offset = part_size\n",
    "        else:\n",
    "            os.remove(tmp_path)\n",
    "\n",
    "    # Open remote and local\n",
    "    with sftp.file(remote_path, \"rb\") as rf:\n",
    "        try:\n",
    "            try:\n",
    "                rf.set_pipelined(True)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            if offset:\n",
    "                rf.seek(offset)\n",
    "\n",
    "            # Append if resuming, write if fresh\n",
    "            with open(tmp_path, \"ab\" if offset else \"wb\") as lf:\n",
    "                transferred = offset\n",
    "\n",
    "                while transferred < remote_size:\n",
    "                    to_read = min(chunk_size, remote_size - transferred)\n",
    "                    data = rf.read(to_read)\n",
    "                    if not data:\n",
    "                        #don't accept short-read silently\n",
    "                        raise IOError(\n",
    "                            f\"Short read at {transferred:,} of {remote_size:,} bytes\"\n",
    "                        )\n",
    "                    lf.write(data)\n",
    "                    transferred += len(data)\n",
    "                    if progress and remote_size:\n",
    "                        print(f\"{transferred / remote_size:.2%} transferred...\")\n",
    "                lf.flush()\n",
    "                os.fsync(lf.fileno())\n",
    "\n",
    "        finally:\n",
    "            # SFTPFile closed by context manager\n",
    "            pass\n",
    "\n",
    "    # Mandatory size verification\n",
    "    local_size = os.path.getsize(tmp_path)\n",
    "    if local_size != remote_size:\n",
    "        raise IOError(\n",
    "            f\"Post-download size mismatch (local {local_size:,}, remote {remote_size:,})\"\n",
    "        )\n",
    "\n",
    "    if verify in {\"sha256\", \"md5\"}:\n",
    "        algo = verify\n",
    "        local_hash = _hash_file(tmp_path, algo=algo)\n",
    "        remote_hash = None\n",
    "        if ssh_for_remote_hash is not None:\n",
    "            remote_hash = _remote_hash(ssh_for_remote_hash, remote_path, algo=algo)\n",
    "\n",
    "        if remote_hash and (remote_hash != local_hash):\n",
    "            # Clean up .part so next run starts fresh\n",
    "            try:\n",
    "                os.remove(tmp_path)\n",
    "            except Exception:\n",
    "                pass\n",
    "            raise IOError(\n",
    "                f\"{algo.upper()} mismatch: local={local_hash} remote={remote_hash}\"\n",
    "            )\n",
    "\n",
    "    # Move atomically into place\n",
    "    os.replace(tmp_path, local_path)\n",
    "    if progress:\n",
    "        print(\"Download complete (atomic & verified).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53f05063-ec80-4a41-9611-641331b7f462",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def download_new_files_and_queue(sftp: paramiko.SFTPClient, df_new):\n",
    "    \"\"\"\n",
    "    Download each new file to /tmp and upsert into pending_ingest_queue.\n",
    "    \"\"\"\n",
    "    os.makedirs(TMP_DIR, exist_ok=True)\n",
    "\n",
    "    # Collect is OK if you expect modest number of files. If you expect thousands, we can paginate and stream.\n",
    "    rows = df_new.select(\n",
    "        \"file_fingerprint\",\n",
    "        \"source_system\",\n",
    "        \"sftp_path\",\n",
    "        \"file_name\",\n",
    "        \"file_size\",\n",
    "        \"file_modified_time\",\n",
    "    ).collect()\n",
    "\n",
    "    queued = []\n",
    "    for r in rows:\n",
    "        fp = r[\"file_fingerprint\"]\n",
    "        sftp_path = r[\"sftp_path\"]\n",
    "        file_name = r[\"file_name\"]\n",
    "\n",
    "        remote_path = f\"{sftp_path.rstrip('/')}/{file_name}\"\n",
    "        local_path = os.path.join(TMP_DIR, f\"{fp}__{file_name}\")\n",
    "\n",
    "        # If local already exists (e.g., rerun), skip re-download\n",
    "        if not os.path.exists(local_path):\n",
    "            print(f\"Downloading new file from SFTP: {remote_path} -> {local_path}\")\n",
    "            logger.info(f\"Downloading new file from SFTP: {remote_path} -> {local_path}\")\n",
    "            #sftp.get(remote_path, local_path)\n",
    "            download_sftp_atomic(sftp, remote_path, local_path, chunk = 150)\n",
    "        else:\n",
    "            print(f\"Skipping download, file already exists: {local_path}\")\n",
    "            logger.info(f\"Local file already staged, skipping download: {local_path}\")\n",
    "\n",
    "        queued.append(\n",
    "            {\n",
    "                \"file_fingerprint\": fp,\n",
    "                \"source_system\": r[\"source_system\"],\n",
    "                \"sftp_path\": sftp_path,\n",
    "                \"file_name\": file_name,\n",
    "                \"file_size\": r[\"file_size\"],\n",
    "                \"file_modified_time\": r[\"file_modified_time\"],\n",
    "                \"local_tmp_path\": local_path,\n",
    "                \"queued_at\": datetime.now(timezone.utc),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    if not queued:\n",
    "        return 0\n",
    "\n",
    "    qschema = T.StructType(\n",
    "        [\n",
    "            T.StructField(\"file_fingerprint\", T.StringType(), False),\n",
    "            T.StructField(\"source_system\", T.StringType(), False),\n",
    "            T.StructField(\"sftp_path\", T.StringType(), False),\n",
    "            T.StructField(\"file_name\", T.StringType(), False),\n",
    "            T.StructField(\"file_size\", T.LongType(), True),\n",
    "            T.StructField(\"file_modified_time\", T.TimestampType(), True),\n",
    "            T.StructField(\"local_tmp_path\", T.StringType(), False),\n",
    "            T.StructField(\"queued_at\", T.TimestampType(), False),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    df_queue = spark.createDataFrame(queued, schema=qschema)\n",
    "    df_queue.createOrReplaceTempView(\"incoming_queue_rows\")\n",
    "\n",
    "    # Upsert into queue (idempotent by fingerprint)\n",
    "\n",
    "    spark.sql(\n",
    "        f\"\"\"\n",
    "        MERGE INTO {QUEUE_TABLE} AS t\n",
    "        USING incoming_queue_rows AS s\n",
    "        ON t.file_fingerprint = s.file_fingerprint\n",
    "        WHEN MATCHED THEN UPDATE SET\n",
    "        t.local_tmp_path = s.local_tmp_path,\n",
    "        t.queued_at = s.queued_at\n",
    "        WHEN NOT MATCHED THEN INSERT *\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "    return len(queued)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80968f66-5082-49ca-b03f-b3a1ef0bb908",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "transport = None\n",
    "sftp = None\n",
    "\n",
    "try:\n",
    "    ensure_tables()\n",
    "\n",
    "    transport, sftp = connect_sftp(host, user, password)\n",
    "    logger.info(f\"Connected to SFTP host={host} and scanning folder={remote_folder}\")\n",
    "\n",
    "    file_rows = list_receive_files(sftp, remote_folder)\n",
    "    if not file_rows:\n",
    "        logger.info(f\"No files found in SFTP folder: {remote_folder}. Exiting (no-op).\")\n",
    "        dbutils.notebook.exit(\"NO_FILES\")\n",
    "\n",
    "    df_listing = build_listing_df(file_rows)\n",
    "\n",
    "    # 1) Ensure everything on SFTP is at least represented in manifest as NEW\n",
    "    upsert_new_to_manifest(df_listing)\n",
    "\n",
    "    # 2) Queue anything that is still NEW and not already queued\n",
    "    df_to_queue = get_files_to_queue(df_listing)\n",
    "\n",
    "    to_queue_count = df_to_queue.count()\n",
    "    if to_queue_count == 0:\n",
    "        logger.info(\"No files to queue: either nothing is NEW, or NEW files are already queued. Exiting (no-op).\")\n",
    "        dbutils.notebook.exit(\"QUEUED_FILES=0\")\n",
    "\n",
    "    logger.info(f\"Queuing {to_queue_count} NEW-unqueued file(s) to {QUEUE_TABLE} and staging locally.\")\n",
    "    queued_count = download_new_files_and_queue(sftp, df_to_queue)\n",
    "\n",
    "    logger.info(f\"Queued {queued_count} file(s) for downstream processing in {QUEUE_TABLE}.\")\n",
    "    dbutils.notebook.exit(f\"QUEUED_FILES={queued_count}\")\n",
    "\n",
    "finally:\n",
    "    try:\n",
    "        if sftp is not None:\n",
    "            sftp.close()\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        if transport is not None:\n",
    "            transport.close()\n",
    "    except Exception:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80a87ce4-8f44-449e-bef7-f40a73e60bf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_sftp_receive_scan",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
