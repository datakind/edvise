{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ed056e5-420d-4b47-8812-cf63f1f895c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# Script 4 — 04_per_institution_bronze_ingest\n",
    "#\n",
    "# Purpose:\n",
    "#   Consume institution_ingest_plan (created by Script 3), and for each (file × institution):\n",
    "#     - get bearer token from SST staging using X-API-KEY (from Databricks secrets)\n",
    "#     - call /api/v1/institutions/pdp-id/{pdp_id} to resolve institution name\n",
    "#     - map name -> schema prefix via databricksify_inst_name()\n",
    "#     - locate <prefix>_bronze schema in staging_sst_02\n",
    "#     - choose a volume in that schema containing \"bronze\"\n",
    "#     - filter rows by institution id (exactly like current script)\n",
    "#     - write to bronze volume using helper.process_and_save_file (exact same ingestion method)\n",
    "#   After all institutions for a file are processed, update ingestion_manifest:\n",
    "#     - BRONZE_WRITTEN if all institution ingests succeeded (or were already present)\n",
    "#     - FAILED if any error occurred for that file (store error_message)\n",
    "#\n",
    "# Constraints:\n",
    "#   - NO SFTP connection (uses staged local files from Script 1/3)\n",
    "#   - Uses existing ingestion function + behavior from current script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de7936c9-a18c-4a87-858a-2c15045481d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install pandas python-box pyyaml requests paramiko\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83538ecc-3986-46a8-a755-fb037fee8039",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import yaml\n",
    "import requests\n",
    "import pandas as pd\n",
    "from box import Box\n",
    "from datetime import datetime, timezone\n",
    "import paramiko\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "from helper import process_and_save_file, CustomLogger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7aea7d3e-2734-40ed-ae5c-a32e67ce3541",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logger = CustomLogger()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# ---------------------------\n",
    "# Config + constants\n",
    "# ---------------------------\n",
    "with open(\"gcp_config.yaml\", \"rb\") as f:\n",
    "    cfg = Box(yaml.safe_load(f))\n",
    "\n",
    "CATALOG = \"staging_sst_01\"\n",
    "DEFAULT_SCHEMA = \"default\"\n",
    "\n",
    "PLAN_TABLE = f\"{CATALOG}.{DEFAULT_SCHEMA}.institution_ingest_plan\"\n",
    "MANIFEST_TABLE = f\"{CATALOG}.{DEFAULT_SCHEMA}.ingestion_manifest\"\n",
    "\n",
    "SST_BASE_URL = \"https://staging-sst.datakind.org\"\n",
    "SST_TOKEN_ENDPOINT = f\"{SST_BASE_URL}/api/v1/token-from-api-key\"\n",
    "INSTITUTION_LOOKUP_PATH = \"/api/v1/institutions/pdp-id/{pdp_id}\"\n",
    "\n",
    "# IMPORTANT: set these two to your actual secret scope + key name(s)\n",
    "SST_SECRET_SCOPE = cfg.institution.secure_assets[\"scope\"]\n",
    "SST_API_KEY_SECRET_KEY = \"sst_staging_api_key\"  # <-- update if your secret key is named differently\n",
    "SST_API_KEY = dbutils.secrets.get(scope=SST_SECRET_SCOPE, key=SST_API_KEY_SECRET_KEY).strip()\n",
    "if not SST_API_KEY:\n",
    "    raise RuntimeError(f\"Empty SST API key from secrets: scope={SST_SECRET_SCOPE} key={SST_API_KEY_SECRET_KEY}\")\n",
    "\n",
    "_session = requests.Session()\n",
    "_session.headers.update({\"accept\": \"application/json\"})\n",
    "\n",
    "_bearer_token = None\n",
    "_institution_cache: dict[str, dict] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0caeea4c-056c-4bd2-9f12-99895d5638a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def output_file_name_from_sftp(file_name: str) -> str:\n",
    "    return f\"{os.path.basename(file_name).split('.')[0]}.csv\"\n",
    "\n",
    "# Column normalization + renames (kept identical to current script)\n",
    "def normalize_col(name: str) -> str:\n",
    "    name = name.strip().lower()\n",
    "    name = re.sub(r\"[^a-z0-9_]\", \"_\", name)\n",
    "    name = re.sub(r\"_+\", \"_\", name)\n",
    "    name = name.strip(\"_\")\n",
    "    return name\n",
    "\n",
    "RENAMES = {\n",
    "    \"attemptedgatewaymathyear1\": \"attempted_gateway_math_year_1\",\n",
    "    \"attemptedgatewayenglishyear1\": \"attempted_gateway_english_year_1\",\n",
    "    \"completedgatewaymathyear1\": \"completed_gateway_math_year_1\",\n",
    "    \"completedgatewayenglishyear1\": \"completed_gateway_english_year_1\",\n",
    "    \"gatewaymathgradey1\": \"gateway_math_grade_y_1\",\n",
    "    \"gatewayenglishgradey1\": \"gateway_english_grade_y_1\",\n",
    "    \"attempteddevmathy1\": \"attempted_dev_math_y_1\",\n",
    "    \"attempteddevenglishy1\": \"attempted_dev_english_y_1\",\n",
    "    \"completeddevmathy1\": \"completed_dev_math_y_1\",\n",
    "    \"completeddevenglishy1\": \"completed_dev_english_y_1\",\n",
    "}\n",
    "\n",
    "# Provided by you\n",
    "def databricksify_inst_name(inst_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Follow DK standardized rules for naming conventions used in Databricks.\n",
    "    \"\"\"\n",
    "    name = inst_name.lower()\n",
    "    dk_replacements = {\n",
    "        \"community technical college\": \"ctc\",\n",
    "        \"community college\": \"cc\",\n",
    "        \"of science and technology\": \"st\",\n",
    "        \"university\": \"uni\",\n",
    "        \"college\": \"col\",\n",
    "    }\n",
    "\n",
    "    for old, new in dk_replacements.items():\n",
    "        name = name.replace(old, new)\n",
    "\n",
    "    special_char_replacements = {\" & \": \" \", \"&\": \" \", \"-\": \" \"}\n",
    "    for old, new in special_char_replacements.items():\n",
    "        name = name.replace(old, new)\n",
    "\n",
    "    final_name = name.replace(\" \", \"_\")\n",
    "\n",
    "    pattern = \"^[a-z0-9_]*$\"\n",
    "    if not re.match(pattern, final_name):\n",
    "        raise ValueError(\"Unexpected character found in Databricks compatible name.\")\n",
    "    return final_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f07cdf2e-5df8-4faf-9046-e05452d988b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def fetch_bearer_token() -> str:\n",
    "    \"\"\"\n",
    "    Fetch bearer token from API key using X-API-KEY header.\n",
    "    Assumes token endpoint returns JSON containing one of: access_token, token, bearer_token, jwt.\n",
    "    \"\"\"\n",
    "    resp = _session.post(\n",
    "        SST_TOKEN_ENDPOINT,\n",
    "        headers={\"accept\": \"application/json\", \"X-API-KEY\": SST_API_KEY},\n",
    "        timeout=30,\n",
    "    )\n",
    "    if resp.status_code == 401:\n",
    "        raise PermissionError(\"Unauthorized calling token endpoint (check X-API-KEY secret).\")\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    data = resp.json()\n",
    "    for k in [\"access_token\", \"token\", \"bearer_token\", \"jwt\"]:\n",
    "        v = data.get(k)\n",
    "        if isinstance(v, str) and v.strip():\n",
    "            return v.strip()\n",
    "\n",
    "    raise ValueError(f\"Token endpoint response missing expected token field. Keys={list(data.keys())}\")\n",
    "\n",
    "def ensure_auth():\n",
    "    global _bearer_token\n",
    "    if _bearer_token is None:\n",
    "        _bearer_token = fetch_bearer_token()\n",
    "        _session.headers.update({\"Authorization\": f\"Bearer {_bearer_token}\"})\n",
    "\n",
    "def refresh_auth():\n",
    "    global _bearer_token\n",
    "    _bearer_token = fetch_bearer_token()\n",
    "    _session.headers.update({\"Authorization\": f\"Bearer {_bearer_token}\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce28afb2-6f19-4a92-935a-49e82c18b317",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def fetch_institution_by_pdp_id(pdp_id: str) -> dict:\n",
    "    \"\"\"\n",
    "    Resolve institution for PDP id. Cached within run.\n",
    "    Refresh token once on 401.\n",
    "    \"\"\"\n",
    "    pid = str(pdp_id).strip()\n",
    "    if pid in _institution_cache:\n",
    "        return _institution_cache[pid]\n",
    "\n",
    "    ensure_auth()\n",
    "\n",
    "    url = SST_BASE_URL + INSTITUTION_LOOKUP_PATH.format(pdp_id=pid)\n",
    "    resp = _session.get(url, timeout=30)\n",
    "\n",
    "    if resp.status_code == 401:\n",
    "        refresh_auth()\n",
    "        resp = _session.get(url, timeout=30)\n",
    "\n",
    "    if resp.status_code == 404:\n",
    "        raise ValueError(f\"Institution PDP ID not found in SST staging: {pid}\")\n",
    "\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    _institution_cache[pid] = data\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6eab61e4-7f7d-498b-8401-93f9c3a2390e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "_schema_cache: set[str] | None = None\n",
    "_bronze_volume_cache: dict[str, str] = {}  # key: f\"{catalog}.{schema}\" -> volume_name\n",
    "\n",
    "def list_schemas_in_catalog(catalog: str) -> set[str]:\n",
    "    global _schema_cache\n",
    "    if _schema_cache is None:\n",
    "        rows = spark.sql(f\"SHOW SCHEMAS IN {catalog}\").collect()\n",
    "        _schema_cache = {r[\"databaseName\"] for r in rows}\n",
    "    return _schema_cache\n",
    "\n",
    "def find_bronze_schema(catalog: str, inst_prefix: str) -> str:\n",
    "    target = f\"{inst_prefix}_bronze\"\n",
    "    schemas = list_schemas_in_catalog(catalog)\n",
    "    if target not in schemas:\n",
    "        raise ValueError(f\"Bronze schema not found: {catalog}.{target}\")\n",
    "    return target\n",
    "\n",
    "def find_bronze_volume_name(catalog: str, schema: str) -> str:\n",
    "    key = f\"{catalog}.{schema}\"\n",
    "    if key in _bronze_volume_cache:\n",
    "        return _bronze_volume_cache[key]\n",
    "\n",
    "    vols = spark.sql(f\"SHOW VOLUMES IN {catalog}.{schema}\").collect()\n",
    "    if not vols:\n",
    "        raise ValueError(f\"No volumes found in {catalog}.{schema}\")\n",
    "\n",
    "    # Usually \"volume_name\", but be defensive\n",
    "    def _get_vol_name(row):\n",
    "        d = row.asDict()\n",
    "        for k in [\"volume_name\", \"volumeName\", \"name\"]:\n",
    "            if k in d:\n",
    "                return d[k]\n",
    "        return list(d.values())[0]\n",
    "\n",
    "    vol_names = [_get_vol_name(v) for v in vols]\n",
    "    bronze_like = [v for v in vol_names if \"bronze\" in v.lower()]\n",
    "    if bronze_like:\n",
    "        _bronze_volume_cache[key] = bronze_like[0]\n",
    "        return bronze_like[0]\n",
    "\n",
    "    raise ValueError(f\"No volume containing 'bronze' found in {catalog}.{schema}. Volumes={vol_names}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11f1eb6c-1bbe-4302-89c7-14c12796ebb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def update_manifest(file_fingerprint: str, status: str, error_message: str | None):\n",
    "    \"\"\"\n",
    "    Update ingestion_manifest for this file_fingerprint.\n",
    "    Assumes Script 1 inserted status=NEW already.\n",
    "    \"\"\"\n",
    "    now_ts = datetime.now(timezone.utc)\n",
    "\n",
    "    # ingested_at only set when we finish BRONZE_WRITTEN\n",
    "    row = {\n",
    "        \"file_fingerprint\": file_fingerprint,\n",
    "        \"status\": status,\n",
    "        \"error_message\": error_message,\n",
    "        \"ingested_at\": now_ts if status == \"BRONZE_WRITTEN\" else None,\n",
    "        \"processed_at\": now_ts,\n",
    "    }\n",
    "\n",
    "    schema = T.StructType(\n",
    "        [\n",
    "            T.StructField(\"file_fingerprint\", T.StringType(), False),\n",
    "            T.StructField(\"status\", T.StringType(), False),\n",
    "            T.StructField(\"error_message\", T.StringType(), True),\n",
    "            T.StructField(\"ingested_at\", T.TimestampType(), True),\n",
    "            T.StructField(\"processed_at\", T.TimestampType(), False),\n",
    "        ]\n",
    "    )\n",
    "    df = spark.createDataFrame([row], schema=schema)\n",
    "    df.createOrReplaceTempView(\"manifest_updates\")\n",
    "\n",
    "    spark.sql(\n",
    "        f\"\"\"\n",
    "        MERGE INTO {MANIFEST_TABLE} AS t\n",
    "        USING manifest_updates AS s\n",
    "        ON t.file_fingerprint = s.file_fingerprint\n",
    "        WHEN MATCHED THEN UPDATE SET\n",
    "          t.status = s.status,\n",
    "          t.error_message = s.error_message,\n",
    "          t.ingested_at = COALESCE(s.ingested_at, t.ingested_at),\n",
    "          t.processed_at = s.processed_at\n",
    "        \"\"\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a0c7f38-ab8f-4a54-a778-6c2e79b5044d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if not spark.catalog.tableExists(PLAN_TABLE):\n",
    "    logger.info(f\"Plan table not found: {PLAN_TABLE}. Exiting (no-op).\")\n",
    "    dbutils.notebook.exit(\"NO_PLAN_TABLE\")\n",
    "\n",
    "if not spark.catalog.tableExists(MANIFEST_TABLE):\n",
    "    raise RuntimeError(f\"Manifest table missing: {MANIFEST_TABLE}\")\n",
    "\n",
    "plan_df = spark.table(PLAN_TABLE)\n",
    "if plan_df.limit(1).count() == 0:\n",
    "    logger.info(\"institution_ingest_plan is empty. Exiting (no-op).\")\n",
    "    dbutils.notebook.exit(\"NO_WORK_ITEMS\")\n",
    "\n",
    "manifest_df = spark.table(MANIFEST_TABLE).select(\"file_fingerprint\", \"status\")\n",
    "plan_new_df = (\n",
    "    plan_df.join(manifest_df, on=\"file_fingerprint\", how=\"inner\")\n",
    "    .where(F.col(\"status\") == F.lit(\"NEW\"))\n",
    ")\n",
    "display(plan_new_df)\n",
    "if plan_new_df.limit(1).count() == 0:\n",
    "    logger.info(\"No planned work items where manifest status=NEW. Exiting (no-op).\")\n",
    "    dbutils.notebook.exit(\"NO_NEW_TO_INGEST\")\n",
    "\n",
    "# Collect file groups\n",
    "file_groups = (\n",
    "    plan_new_df.select(\n",
    "        \"file_fingerprint\",\n",
    "        \"file_name\",\n",
    "        \"local_path\",\n",
    "        \"inst_col\",\n",
    "        \"file_size\",\n",
    "        \"file_modified_time\",\n",
    "    )\n",
    "    .distinct()\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "logger.info(f\"Preparing to ingest {len(file_groups)} NEW file(s).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf0729e1-7a4f-402a-85b6-1bca3696f878",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Main per-file ingest loop\n",
    "# ---------------------------\n",
    "processed_files = 0\n",
    "failed_files = 0\n",
    "skipped_files = 0\n",
    "\n",
    "for fg in file_groups:\n",
    "    fp = fg[\"file_fingerprint\"]\n",
    "    sftp_file_name = fg[\"file_name\"]\n",
    "    local_path = fg[\"local_path\"]\n",
    "    inst_col = fg[\"inst_col\"]\n",
    "\n",
    "    if not local_path or not os.path.exists(local_path):\n",
    "        err = f\"Staged local file missing for fp={fp}: {local_path}\"\n",
    "        logger.error(err)\n",
    "        update_manifest(fp, status=\"FAILED\", error_message=err[:8000])\n",
    "        failed_files += 1\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        df_full = pd.read_csv(local_path, on_bad_lines=\"warn\")\n",
    "        df_full = df_full.rename(columns={c: normalize_col(c) for c in df_full.columns})\n",
    "        df_full = df_full.rename(columns=RENAMES)\n",
    "\n",
    "        if inst_col not in df_full.columns:\n",
    "            err = f\"Expected institution column '{inst_col}' not found after normalization/renames for file={sftp_file_name} fp={fp}\"\n",
    "            logger.error(err)\n",
    "            update_manifest(fp, status=\"FAILED\", error_message=err[:8000])\n",
    "            failed_files += 1\n",
    "            continue\n",
    "\n",
    "        inst_ids = (\n",
    "            plan_new_df.where(F.col(\"file_fingerprint\") == fp)\n",
    "            .select(\"institution_id\")\n",
    "            .distinct()\n",
    "            .collect()\n",
    "        )\n",
    "        inst_ids = [r[\"institution_id\"] for r in inst_ids]\n",
    "\n",
    "        if not inst_ids:\n",
    "            logger.info(f\"No institution_ids in plan for file={sftp_file_name} fp={fp}. Marking BRONZE_WRITTEN (no-op).\")\n",
    "            update_manifest(fp, status=\"BRONZE_WRITTEN\", error_message=None)\n",
    "            skipped_files += 1\n",
    "            continue\n",
    "\n",
    "        # Aggregate errors at file-level\n",
    "        file_errors = []\n",
    "\n",
    "        for inst_id in inst_ids:\n",
    "            try:\n",
    "                filtered_df = df_full[df_full[inst_col] == int(inst_id)].reset_index(drop=True)\n",
    "\n",
    "                if filtered_df.empty:\n",
    "                    logger.info(f\"file={sftp_file_name} fp={fp}: institution {inst_id} has 0 rows; skipping.\")\n",
    "                    continue\n",
    "\n",
    "                # Resolve institution -> name\n",
    "                inst_info = fetch_institution_by_pdp_id(inst_id)\n",
    "                inst_name = inst_info.get(\"name\")\n",
    "                if not inst_name:\n",
    "                    raise ValueError(f\"SST API returned no 'name' for pdp_id={inst_id}. Response={inst_info}\")\n",
    "\n",
    "                inst_prefix = databricksify_inst_name(inst_name)\n",
    "\n",
    "                # Find bronze schema + volume\n",
    "                bronze_schema = find_bronze_schema(CATALOG, inst_prefix)\n",
    "                bronze_volume_name = find_bronze_volume_name(CATALOG, bronze_schema)\n",
    "                volume_dir = f\"/Volumes/{CATALOG}/{bronze_schema}/{bronze_volume_name}\"\n",
    "\n",
    "                # Output naming rule (same as current script)\n",
    "                out_file_name = output_file_name_from_sftp(sftp_file_name)\n",
    "                full_path = os.path.join(volume_dir, out_file_name)\n",
    "\n",
    "                # Idempotency check\n",
    "                if os.path.exists(full_path):\n",
    "                    logger.info(f\"file={sftp_file_name} inst={inst_id}: already exists in {volume_dir}; skipping write.\")\n",
    "                    continue\n",
    "\n",
    "                logger.info(f\"file={sftp_file_name} inst={inst_id}: writing to {volume_dir} as {out_file_name}\")\n",
    "                process_and_save_file(volume_dir=volume_dir, file_name=out_file_name, df=filtered_df)\n",
    "                logger.info(f\"file={sftp_file_name} inst={inst_id}: write complete.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                msg = f\"inst_ingest_failed file={sftp_file_name} fp={fp} inst={inst_id}: {e}\"\n",
    "                logger.exception(msg)\n",
    "                file_errors.append(msg)\n",
    "\n",
    "        if file_errors:\n",
    "            err = \" | \".join(file_errors)[:8000]\n",
    "            update_manifest(fp, status=\"FAILED\", error_message=err)\n",
    "            failed_files += 1\n",
    "        else:\n",
    "            update_manifest(fp, status=\"BRONZE_WRITTEN\", error_message=None)\n",
    "            processed_files += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        msg = f\"fatal_file_error file={sftp_file_name} fp={fp}: {e}\"\n",
    "        logger.exception(msg)\n",
    "        update_manifest(fp, status=\"FAILED\", error_message=msg[:8000])\n",
    "        failed_files += 1\n",
    "\n",
    "logger.info(f\"Done. processed_files={processed_files}, failed_files={failed_files}, skipped_files={skipped_files}\")\n",
    "dbutils.notebook.exit(f\"PROCESSED={processed_files};FAILED={failed_files};SKIPPED={skipped_files}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "845210e6-9608-46fe-99de-1c49eb7feb84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_per_institution_bronze_ingest",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
