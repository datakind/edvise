{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ed056e5-420d-4b47-8812-cf63f1f895c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# Script 4 — 04_per_institution_bronze_ingest\n",
    "#\n",
    "# Purpose:\n",
    "#   Consume institution_ingest_plan (created by Script 3), and for each (file × institution):\n",
    "#     - get bearer token from SST staging using X-API-KEY (from Databricks secrets)\n",
    "#     - call /api/v1/institutions/pdp-id/{pdp_id} to resolve institution name\n",
    "#     - map name -> schema prefix via databricksify_inst_name()\n",
    "#     - locate <prefix>_bronze schema in staging_sst_01\n",
    "#     - choose a volume in that schema containing \"bronze\"\n",
    "#     - filter rows by institution id (exactly like current script)\n",
    "#     - write to bronze volume using helper.process_and_save_file (exact same ingestion method)\n",
    "#   After all institutions for a file are processed, update ingestion_manifest:\n",
    "#     - BRONZE_WRITTEN if all institution ingests succeeded (or were already present)\n",
    "#     - FAILED if any error occurred for that file (store error_message)\n",
    "#\n",
    "# Constraints:\n",
    "#   - NO SFTP connection (uses staged local files from Script 1/3)\n",
    "#   - Uses existing ingestion function + behavior from current script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de7936c9-a18c-4a87-858a-2c15045481d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install pandas python-box pyyaml requests paramiko\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83538ecc-3986-46a8-a755-fb037fee8039",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "import pandas as pd\n",
    "from box import Box\n",
    "from databricks.connect import DatabricksSession\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from edvise.utils.api_requests import (\n",
    "    EdviseAPIClient,\n",
    "    databricksify_inst_name,\n",
    "    fetch_institution_by_pdp_id,\n",
    ")\n",
    "from edvise.utils.data_cleaning import convert_to_snake_case\n",
    "from edvise.ingestion.nsc_sftp_helpers import (\n",
    "    find_bronze_schema,\n",
    "    find_bronze_volume_name,\n",
    "    output_file_name_from_sftp,\n",
    "    process_and_save_file,\n",
    "    update_manifest,\n",
    ")\n",
    "from edvise.ingestion.constants import (\n",
    "    CATALOG,\n",
    "    PLAN_TABLE_PATH,\n",
    "    MANIFEST_TABLE_PATH,\n",
    "    SST_BASE_URL,\n",
    "    SST_TOKEN_ENDPOINT,\n",
    "    INSTITUTION_LOOKUP_PATH,\n",
    "    SST_API_KEY_SECRET_KEY,\n",
    "    COLUMN_RENAMES,\n",
    ")\n",
    "\n",
    "try:\n",
    "    dbutils  # noqa: F821\n",
    "except NameError:\n",
    "    from unittest.mock import MagicMock\n",
    "\n",
    "    dbutils = MagicMock()\n",
    "\n",
    "try:\n",
    "    display  # noqa: F821\n",
    "except NameError:\n",
    "\n",
    "    def display(x):\n",
    "        return x\n",
    "\n",
    "\n",
    "spark = DatabricksSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7aea7d3e-2734-40ed-ae5c-a32e67ce3541",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# ---------------------------\n",
    "# Config + constants\n",
    "# ---------------------------\n",
    "with open(\"gcp_config.yaml\", \"rb\") as f:\n",
    "    cfg = Box(yaml.safe_load(f))\n",
    "\n",
    "CATALOG = \"staging_sst_01\"\n",
    "DEFAULT_SCHEMA = \"default\"\n",
    "\n",
    "PLAN_TABLE = f\"{CATALOG}.{DEFAULT_SCHEMA}.institution_ingest_plan\"\n",
    "MANIFEST_TABLE = f\"{CATALOG}.{DEFAULT_SCHEMA}.ingestion_manifest\"\n",
    "\n",
    "SST_BASE_URL = \"https://staging-sst.datakind.org\"\n",
    "SST_TOKEN_ENDPOINT = f\"{SST_BASE_URL}/api/v1/token-from-api-key\"\n",
    "INSTITUTION_LOOKUP_PATH = \"/api/v1/institutions/pdp-id/{pdp_id}\"\n",
    "\n",
    "# IMPORTANT: set these two to your actual secret scope + key name(s)\n",
    "SST_SECRET_SCOPE = cfg.institution.secure_assets[\"scope\"]\n",
    "SST_API_KEY_SECRET_KEY = (\n",
    "    \"sst_staging_api_key\"  # <-- update if your secret key is named differently\n",
    ")\n",
    "SST_API_KEY = dbutils.secrets.get(\n",
    "    scope=SST_SECRET_SCOPE, key=SST_API_KEY_SECRET_KEY\n",
    ").strip()\n",
    "if not SST_API_KEY:\n",
    "    raise RuntimeError(\n",
    "        f\"Empty SST API key from secrets: scope={SST_SECRET_SCOPE} key={SST_API_KEY_SECRET_KEY}\"\n",
    "    )\n",
    "\n",
    "api_client = EdviseAPIClient(\n",
    "    api_key=SST_API_KEY,\n",
    "    base_url=SST_BASE_URL,\n",
    "    token_endpoint=SST_TOKEN_ENDPOINT,\n",
    "    institution_lookup_path=INSTITUTION_LOOKUP_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0caeea4c-056c-4bd2-9f12-99895d5638a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f07cdf2e-5df8-4faf-9046-e05452d988b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce28afb2-6f19-4a92-935a-49e82c18b317",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6eab61e4-7f7d-498b-8401-93f9c3a2390e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11f1eb6c-1bbe-4302-89c7-14c12796ebb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a0c7f38-ab8f-4a54-a778-6c2e79b5044d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if not spark.catalog.tableExists(PLAN_TABLE_PATH):\n",
    "    logger.info(f\"Plan table not found: {PLAN_TABLE_PATH}. Exiting (no-op).\")\n",
    "    dbutils.notebook.exit(\"NO_PLAN_TABLE\")\n",
    "\n",
    "if not spark.catalog.tableExists(MANIFEST_TABLE_PATH):\n",
    "    raise RuntimeError(f\"Manifest table missing: {MANIFEST_TABLE_PATH}\")\n",
    "\n",
    "plan_df = spark.table(PLAN_TABLE_PATH)\n",
    "if plan_df.limit(1).count() == 0:\n",
    "    logger.info(\"institution_ingest_plan is empty. Exiting (no-op).\")\n",
    "    dbutils.notebook.exit(\"NO_WORK_ITEMS\")\n",
    "\n",
    "manifest_df = spark.table(MANIFEST_TABLE_PATH).select(\"file_fingerprint\", \"status\")\n",
    "plan_new_df = plan_df.join(manifest_df, on=\"file_fingerprint\", how=\"inner\").where(\n",
    "    F.col(\"status\") == F.lit(\"NEW\")\n",
    ")\n",
    "display(plan_new_df)\n",
    "if plan_new_df.limit(1).count() == 0:\n",
    "    logger.info(\"No planned work items where manifest status=NEW. Exiting (no-op).\")\n",
    "    dbutils.notebook.exit(\"NO_NEW_TO_INGEST\")\n",
    "\n",
    "# Collect file groups\n",
    "file_groups = (\n",
    "    plan_new_df.select(\n",
    "        \"file_fingerprint\",\n",
    "        \"file_name\",\n",
    "        \"local_path\",\n",
    "        \"inst_col\",\n",
    "        \"file_size\",\n",
    "        \"file_modified_time\",\n",
    "    )\n",
    "    .distinct()\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "logger.info(f\"Preparing to ingest {len(file_groups)} NEW file(s).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf0729e1-7a4f-402a-85b6-1bca3696f878",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Main per-file ingest loop\n",
    "# ---------------------------\n",
    "processed_files = 0\n",
    "failed_files = 0\n",
    "skipped_files = 0\n",
    "\n",
    "for fg in file_groups:\n",
    "    fp = fg[\"file_fingerprint\"]\n",
    "    sftp_file_name = fg[\"file_name\"]\n",
    "    local_path = fg[\"local_path\"]\n",
    "    inst_col = fg[\"inst_col\"]\n",
    "\n",
    "    if not local_path or not os.path.exists(local_path):\n",
    "        err = f\"Staged local file missing for fp={fp}: {local_path}\"\n",
    "        logger.error(err)\n",
    "        update_manifest(\n",
    "            spark, MANIFEST_TABLE, fp, status=\"FAILED\", error_message=err[:8000]\n",
    "        )\n",
    "        failed_files += 1\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        df_full = pd.read_csv(local_path, on_bad_lines=\"warn\")\n",
    "        df_full = df_full.rename(columns={c: convert_to_snake_case(c) for c in df_full.columns})\n",
    "        df_full = df_full.rename(columns=RENAMES)\n",
    "\n",
    "        if inst_col not in df_full.columns:\n",
    "            err = f\"Expected institution column '{inst_col}' not found after normalization/renames for file={sftp_file_name} fp={fp}\"\n",
    "            logger.error(err)\n",
    "            update_manifest(\n",
    "                spark, MANIFEST_TABLE, fp, status=\"FAILED\", error_message=err[:8000]\n",
    "            )\n",
    "            failed_files += 1\n",
    "            continue\n",
    "\n",
    "        # Only cast institution ID column to string (leave other columns as inferred)\n",
    "        df_full[inst_col] = df_full[inst_col].astype(str)\n",
    "\n",
    "        inst_ids = (\n",
    "            plan_new_df.where(F.col(\"file_fingerprint\") == fp)\n",
    "            .select(\"institution_id\")\n",
    "            .distinct()\n",
    "            .collect()\n",
    "        )\n",
    "        inst_ids = [r[\"institution_id\"] for r in inst_ids]\n",
    "\n",
    "        if not inst_ids:\n",
    "            logger.info(\n",
    "                f\"No institution_ids in plan for file={sftp_file_name} fp={fp}. Marking BRONZE_WRITTEN (no-op).\"\n",
    "            )\n",
    "            update_manifest(\n",
    "                spark, MANIFEST_TABLE, fp, status=\"BRONZE_WRITTEN\", error_message=None\n",
    "            )\n",
    "            skipped_files += 1\n",
    "            continue\n",
    "\n",
    "        # Aggregate errors at file-level\n",
    "        file_errors = []\n",
    "\n",
    "        for inst_id in inst_ids:\n",
    "            try:\n",
    "                target_inst_id = str(inst_id)\n",
    "                filtered_df = df_full[df_full[inst_col] == target_inst_id].reset_index(\n",
    "                    drop=True\n",
    "                )\n",
    "\n",
    "                if filtered_df.empty:\n",
    "                    logger.info(\n",
    "                        f\"file={sftp_file_name} fp={fp}: institution {inst_id} has 0 rows; skipping.\"\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "                # Resolve institution -> name\n",
    "                inst_info = fetch_institution_by_pdp_id(api_client, inst_id)\n",
    "                inst_name = inst_info.get(\"name\")\n",
    "                if not inst_name:\n",
    "                    raise ValueError(\n",
    "                        f\"SST API returned no 'name' for pdp_id={inst_id}. Response={inst_info}\"\n",
    "                    )\n",
    "\n",
    "                inst_prefix = databricksify_inst_name(inst_name)\n",
    "\n",
    "                # Find bronze schema + volume\n",
    "                bronze_schema = find_bronze_schema(spark, CATALOG, inst_prefix)\n",
    "                bronze_volume_name = find_bronze_volume_name(\n",
    "                    spark, CATALOG, bronze_schema\n",
    "                )\n",
    "                volume_dir = f\"/Volumes/{CATALOG}/{bronze_schema}/{bronze_volume_name}\"\n",
    "\n",
    "                # Output naming rule (same as current script)\n",
    "                out_file_name = output_file_name_from_sftp(sftp_file_name)\n",
    "                full_path = os.path.join(volume_dir, out_file_name)\n",
    "\n",
    "                # Idempotency check\n",
    "                if os.path.exists(full_path):\n",
    "                    logger.info(\n",
    "                        f\"file={sftp_file_name} inst={inst_id}: already exists in {volume_dir}; skipping write.\"\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "                logger.info(\n",
    "                    f\"file={sftp_file_name} inst={inst_id}: writing to {volume_dir} as {out_file_name}\"\n",
    "                )\n",
    "                process_and_save_file(\n",
    "                    volume_dir=volume_dir, file_name=out_file_name, df=filtered_df\n",
    "                )\n",
    "                logger.info(f\"file={sftp_file_name} inst={inst_id}: write complete.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                msg = f\"inst_ingest_failed file={sftp_file_name} fp={fp} inst={inst_id}: {e}\"\n",
    "                logger.exception(msg)\n",
    "                file_errors.append(msg)\n",
    "\n",
    "        if file_errors:\n",
    "            err = \" | \".join(file_errors)[:8000]\n",
    "            update_manifest(\n",
    "                spark, MANIFEST_TABLE, fp, status=\"FAILED\", error_message=err\n",
    "            )\n",
    "            failed_files += 1\n",
    "        else:\n",
    "            update_manifest(\n",
    "                spark, MANIFEST_TABLE, fp, status=\"BRONZE_WRITTEN\", error_message=None\n",
    "            )\n",
    "            processed_files += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        msg = f\"fatal_file_error file={sftp_file_name} fp={fp}: {e}\"\n",
    "        logger.exception(msg)\n",
    "        update_manifest(\n",
    "            spark, MANIFEST_TABLE, fp, status=\"FAILED\", error_message=msg[:8000]\n",
    "        )\n",
    "        failed_files += 1\n",
    "\n",
    "logger.info(\n",
    "    f\"Done. processed_files={processed_files}, failed_files={failed_files}, skipped_files={skipped_files}\"\n",
    ")\n",
    "dbutils.notebook.exit(\n",
    "    f\"PROCESSED={processed_files};FAILED={failed_files};SKIPPED={skipped_files}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "845210e6-9608-46fe-99de-1c49eb7feb84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_per_institution_bronze_ingest",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
