{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read each *staged* local file (from `pending_ingest_queue`), detect the institution id column,\n",
    "# 2. extract unique institution IDs, and emit per-institution work items.\n",
    "\n",
    "# Constraints:\n",
    "# - NO SFTP connection\n",
    "# - NO API calls\n",
    "# - NO volume writes\n",
    "\n",
    "#Input table:\n",
    "#- `staging_sst_01.default.pending_ingest_queue`\n",
    "\n",
    "#Output table:\n",
    "#- `staging_sst_01.default.institution_ingest_plan`\n",
    "#- Columns: `file_fingerprint`, `file_name`, `local_path`, `institution_id`, `inst_col`, `file_size`, `file_modified_time`, `planned_at`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "679b2064-2a15-4d89-abda-5e9c0148ff61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install pandas python-box pyyaml paramiko\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62608829-5027-4075-a4fc-1e4afc36ef3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import yaml\n",
    "from box import Box\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from databricks.connect import DatabricksSession\n",
    "\n",
    "from helper import CustomLogger, ensure_plan_table, extract_institution_ids\n",
    "\n",
    "try:\n",
    "    dbutils  # noqa: F821\n",
    "except NameError:\n",
    "    from unittest.mock import MagicMock\n",
    "\n",
    "    dbutils = MagicMock()\n",
    "spark = DatabricksSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64156fce-07a6-4eb6-8612-6b29bc06edfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logger = CustomLogger()\n",
    "\n",
    "# Config (kept consistent with prior notebooks)\n",
    "with open(\"gcp_config.yaml\", \"rb\") as f:\n",
    "    _cfg = Box(yaml.safe_load(f))\n",
    "\n",
    "CATALOG = \"staging_sst_01\"\n",
    "DEFAULT_SCHEMA = \"default\"\n",
    "\n",
    "QUEUE_TABLE = f\"{CATALOG}.{DEFAULT_SCHEMA}.pending_ingest_queue\"\n",
    "PLAN_TABLE = f\"{CATALOG}.{DEFAULT_SCHEMA}.institution_ingest_plan\"\n",
    "\n",
    "logger.info(\"Loaded config and initialized logger.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61dd2548-1ed7-4e50-b2c5-3a447d102ec7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# moved to helper.py: ensure_plan_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4abcbd9-8522-4166-a052-7cea2062338b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# moved to helper.py: normalize_col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6374e96c-7cd3-4f14-9ac8-a8183b6a91fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Same hard-coded renames from the current script (kept identical)\n",
    "RENAMES = {\n",
    "    \"attemptedgatewaymathyear1\": \"attempted_gateway_math_year_1\",\n",
    "    \"attemptedgatewayenglishyear1\": \"attempted_gateway_english_year_1\",\n",
    "    \"completedgatewaymathyear1\": \"completed_gateway_math_year_1\",\n",
    "    \"completedgatewayenglishyear1\": \"completed_gateway_english_year_1\",\n",
    "    \"gatewaymathgradey1\": \"gateway_math_grade_y_1\",\n",
    "    \"gatewayenglishgradey1\": \"gateway_english_grade_y_1\",\n",
    "    \"attempteddevmathy1\": \"attempted_dev_math_y_1\",\n",
    "    \"attempteddevenglishy1\": \"attempted_dev_english_y_1\",\n",
    "    \"completeddevmathy1\": \"completed_dev_math_y_1\",\n",
    "    \"completeddevenglishy1\": \"completed_dev_english_y_1\",\n",
    "}\n",
    "\n",
    "INST_COL_PATTERN = re.compile(r\"(?=.*institution)(?=.*id)\", re.IGNORECASE)\n",
    "\n",
    "# moved to helper.py: detect_institution_column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16f879d8-8946-4f70-8e36-143ed334d25b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# moved to helper.py: extract_institution_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87047914-fec0-4f35-b33f-d1b927605d11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ensure_plan_table(spark, PLAN_TABLE)\n",
    "\n",
    "# Pull queued staged files (Script 1 output)\n",
    "if not spark.catalog.tableExists(QUEUE_TABLE):\n",
    "    logger.info(f\"Queue table {QUEUE_TABLE} not found. Exiting (no-op).\")\n",
    "    dbutils.notebook.exit(\"NO_QUEUE_TABLE\")\n",
    "\n",
    "queue_df = spark.read.table(QUEUE_TABLE)\n",
    "\n",
    "if queue_df.limit(1).count() == 0:\n",
    "    logger.info(\"pending_ingest_queue is empty. Exiting (no-op).\")\n",
    "    dbutils.notebook.exit(\"NO_QUEUED_FILES\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21683394-0bec-42b8-82dd-1a4590519de5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Avoid regenerating plans for files already expanded\n",
    "existing_fp = (\n",
    "    spark.table(PLAN_TABLE).select(\"file_fingerprint\").distinct()\n",
    "    if spark.catalog.tableExists(PLAN_TABLE)\n",
    "    else None\n",
    ")\n",
    "if existing_fp is not None:\n",
    "    queue_df = queue_df.join(existing_fp, on=\"file_fingerprint\", how=\"left_anti\")\n",
    "\n",
    "if queue_df.limit(1).count() == 0:\n",
    "    logger.info(\n",
    "        \"All queued files have already been expanded into institution work items. Exiting (no-op).\"\n",
    "    )\n",
    "    dbutils.notebook.exit(\"NO_NEW_EXPANSION_WORK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "540c7880-f14a-4607-979a-856f17066c50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "queued_files = queue_df.select(\n",
    "    \"file_fingerprint\",\n",
    "    \"file_name\",\n",
    "    F.col(\"local_tmp_path\").alias(\"local_path\"),\n",
    "    \"file_size\",\n",
    "    \"file_modified_time\",\n",
    ").collect()\n",
    "\n",
    "logger.info(\n",
    "    f\"Expanding {len(queued_files)} staged file(s) into per-institution work items...\"\n",
    ")\n",
    "\n",
    "work_items = []\n",
    "missing_files = []\n",
    "\n",
    "for r in queued_files:\n",
    "    fp = r[\"file_fingerprint\"]\n",
    "    file_name = r[\"file_name\"]\n",
    "    local_path = r[\"local_path\"]\n",
    "\n",
    "    if not local_path or not os.path.exists(local_path):\n",
    "        missing_files.append((fp, file_name, local_path))\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        inst_col, inst_ids = extract_institution_ids(\n",
    "            local_path, renames=RENAMES, inst_col_pattern=INST_COL_PATTERN\n",
    "        )\n",
    "        if inst_col is None:\n",
    "            logger.warning(\n",
    "                f\"No institution id column found for file={file_name} fp={fp}. Skipping this file.\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        if not inst_ids:\n",
    "            logger.warning(\n",
    "                f\"Institution column found but no IDs present for file={file_name} fp={fp}. Skipping.\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        now_ts = datetime.now(timezone.utc)\n",
    "        for inst_id in inst_ids:\n",
    "            work_items.append(\n",
    "                {\n",
    "                    \"file_fingerprint\": fp,\n",
    "                    \"file_name\": file_name,\n",
    "                    \"local_path\": local_path,\n",
    "                    \"institution_id\": inst_id,\n",
    "                    \"inst_col\": inst_col,\n",
    "                    \"file_size\": r[\"file_size\"],\n",
    "                    \"file_modified_time\": r[\"file_modified_time\"],\n",
    "                    \"planned_at\": now_ts,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        logger.info(\n",
    "            f\"file={file_name} fp={fp}: found {len(inst_ids)} institution id(s) using column '{inst_col}'\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Failed expanding file={file_name} fp={fp}: {e}\")\n",
    "        # We don't write manifests here per your division; fail fast so workflow can surface issue.\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32d5bc9c-16a1-42b4-adef-f1a442e5d447",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if missing_files:\n",
    "    # This usually indicates the cluster changed or /tmp was cleared.\n",
    "    # Fail fast so the workflow stops (downstream cannot proceed without the staged files).\n",
    "    msg = (\n",
    "        \"Some staged files are missing on disk (likely /tmp cleared or different cluster). \"\n",
    "        + \"; \".join([f\"fp={fp} file={fn} path={lp}\" for fp, fn, lp in missing_files])\n",
    "    )\n",
    "    logger.error(msg)\n",
    "    raise FileNotFoundError(msg)\n",
    "\n",
    "if not work_items:\n",
    "    logger.info(\"No work items generated from staged files. Exiting (no-op).\")\n",
    "    dbutils.notebook.exit(\"NO_WORK_ITEMS\")\n",
    "\n",
    "schema = T.StructType(\n",
    "    [\n",
    "        T.StructField(\"file_fingerprint\", T.StringType(), False),\n",
    "        T.StructField(\"file_name\", T.StringType(), False),\n",
    "        T.StructField(\"local_path\", T.StringType(), False),\n",
    "        T.StructField(\"institution_id\", T.StringType(), False),\n",
    "        T.StructField(\"inst_col\", T.StringType(), False),\n",
    "        T.StructField(\"file_size\", T.LongType(), True),\n",
    "        T.StructField(\"file_modified_time\", T.TimestampType(), True),\n",
    "        T.StructField(\"planned_at\", T.TimestampType(), False),\n",
    "    ]\n",
    ")\n",
    "\n",
    "df_plan = spark.createDataFrame(work_items, schema=schema)\n",
    "df_plan.createOrReplaceTempView(\"incoming_plan_rows\")\n",
    "\n",
    "# Idempotent upsert: unique per (file_fingerprint, institution_id)\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    MERGE INTO {PLAN_TABLE} AS t\n",
    "    USING incoming_plan_rows AS s\n",
    "    ON  t.file_fingerprint = s.file_fingerprint\n",
    "    AND t.institution_id   = s.institution_id\n",
    "    WHEN MATCHED THEN UPDATE SET\n",
    "      t.file_name          = s.file_name,\n",
    "      t.local_path         = s.local_path,\n",
    "      t.inst_col           = s.inst_col,\n",
    "      t.file_size          = s.file_size,\n",
    "      t.file_modified_time = s.file_modified_time,\n",
    "      t.planned_at         = s.planned_at\n",
    "    WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "count_out = df_plan.count()\n",
    "logger.info(f\"Wrote/updated {count_out} institution work item(s) into {PLAN_TABLE}.\")\n",
    "dbutils.notebook.exit(f\"WORK_ITEMS={count_out}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc228f6a-2fb6-4a76-a573-07f91b0f551f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_file_institution_expand",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
