{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read each *staged* local file (from `pending_ingest_queue`), detect the institution id column,\n",
    "# 2. extract unique institution IDs, and emit per-institution work items.\n",
    "\n",
    "# Constraints:\n",
    "# - NO SFTP connection\n",
    "# - NO API calls\n",
    "# - NO volume writes\n",
    "\n",
    "# Input table:\n",
    "# - `staging_sst_01.default.pending_ingest_queue`\n",
    "\n",
    "# Output table:\n",
    "# - `staging_sst_01.default.institution_ingest_plan`\n",
    "# - Columns: `file_fingerprint`, `file_name`, `local_path`, `institution_id`, `inst_col`, `file_size`, `file_modified_time`, `planned_at`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "679b2064-2a15-4d89-abda-5e9c0148ff61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install pandas python-box pyyaml paramiko\n",
    "%pip install git+https://github.com/datakind/edvise.git@Automated_Ingestion_Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62608829-5027-4075-a4fc-1e4afc36ef3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from databricks.connect import DatabricksSession\n",
    "\n",
    "from edvise.ingestion.nsc_sftp_helpers import ensure_plan_table, extract_institution_ids\n",
    "from edvise.ingestion.constants import (\n",
    "    QUEUE_TABLE_PATH,\n",
    "    PLAN_TABLE_PATH,\n",
    "    COLUMN_RENAMES,\n",
    "    INSTITUTION_COLUMN_PATTERN,\n",
    ")\n",
    "\n",
    "try:\n",
    "    dbutils  # noqa: F821\n",
    "except NameError:\n",
    "    from unittest.mock import MagicMock\n",
    "\n",
    "    dbutils = MagicMock()\n",
    "spark = DatabricksSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64156fce-07a6-4eb6-8612-6b29bc06edfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "INST_COL_PATTERN = re.compile(INSTITUTION_COLUMN_PATTERN, re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87047914-fec0-4f35-b33f-d1b927605d11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ensure_plan_table(spark, PLAN_TABLE_PATH)\n",
    "\n",
    "# Pull queued staged files (Script 1 output)\n",
    "if not spark.catalog.tableExists(QUEUE_TABLE_PATH):\n",
    "    logger.info(f\"Queue table {QUEUE_TABLE_PATH} not found. Exiting (no-op).\")\n",
    "    dbutils.notebook.exit(\"NO_QUEUE_TABLE\")\n",
    "\n",
    "queue_df = spark.read.table(QUEUE_TABLE_PATH)\n",
    "\n",
    "if queue_df.limit(1).count() == 0:\n",
    "    logger.info(\"pending_ingest_queue is empty. Exiting (no-op).\")\n",
    "    dbutils.notebook.exit(\"NO_QUEUED_FILES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21683394-0bec-42b8-82dd-1a4590519de5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Avoid regenerating plans for files already expanded\n",
    "existing_fp = (\n",
    "    spark.table(PLAN_TABLE_PATH).select(\"file_fingerprint\").distinct()\n",
    "    if spark.catalog.tableExists(PLAN_TABLE_PATH)\n",
    "    else None\n",
    ")\n",
    "if existing_fp is not None:\n",
    "    queue_df = queue_df.join(existing_fp, on=\"file_fingerprint\", how=\"left_anti\")\n",
    "\n",
    "if queue_df.limit(1).count() == 0:\n",
    "    logger.info(\n",
    "        \"All queued files have already been expanded into institution work items. Exiting (no-op).\"\n",
    "    )\n",
    "    dbutils.notebook.exit(\"NO_NEW_EXPANSION_WORK\")\n",
    "\n",
    "logger.info(\"Queued files to expand preview (after excluding already-expanded):\")\n",
    "queue_df.select(\"file_fingerprint\", \"file_name\", \"local_tmp_path\", \"queued_at\").show(\n",
    "    25, truncate=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "540c7880-f14a-4607-979a-856f17066c50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "queued_files = queue_df.select(\n",
    "    \"file_fingerprint\",\n",
    "    \"file_name\",\n",
    "    F.col(\"local_tmp_path\").alias(\"local_path\"),\n",
    "    \"file_size\",\n",
    "    \"file_modified_time\",\n",
    ").collect()\n",
    "\n",
    "logger.info(\n",
    "    f\"Expanding {len(queued_files)} staged file(s) into per-institution work items...\"\n",
    ")\n",
    "\n",
    "work_items = []\n",
    "missing_files = []\n",
    "\n",
    "for r in queued_files:\n",
    "    fp = r[\"file_fingerprint\"]\n",
    "    file_name = r[\"file_name\"]\n",
    "    local_path = r[\"local_path\"]\n",
    "\n",
    "    if not local_path or not os.path.exists(local_path):\n",
    "        missing_files.append((fp, file_name, local_path))\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        inst_col, inst_ids = extract_institution_ids(\n",
    "            local_path, renames=COLUMN_RENAMES, inst_col_pattern=INST_COL_PATTERN\n",
    "        )\n",
    "        if inst_col is None:\n",
    "            logger.warning(\n",
    "                f\"No institution id column found for file={file_name} fp={fp}. Skipping this file.\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        if not inst_ids:\n",
    "            logger.warning(\n",
    "                f\"Institution column found but no IDs present for file={file_name} fp={fp}. Skipping.\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        now_ts = datetime.now(timezone.utc)\n",
    "        for inst_id in inst_ids:\n",
    "            work_items.append(\n",
    "                {\n",
    "                    \"file_fingerprint\": fp,\n",
    "                    \"file_name\": file_name,\n",
    "                    \"local_path\": local_path,\n",
    "                    \"institution_id\": inst_id,\n",
    "                    \"inst_col\": inst_col,\n",
    "                    \"file_size\": r[\"file_size\"],\n",
    "                    \"file_modified_time\": r[\"file_modified_time\"],\n",
    "                    \"planned_at\": now_ts,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        preview_ids = inst_ids[:10]\n",
    "        logger.info(\n",
    "            f\"file={file_name} fp={fp}: found {len(inst_ids)} institution id(s) using column '{inst_col}'. \"\n",
    "            f\"Preview first 10 IDs={preview_ids}\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Failed expanding file={file_name} fp={fp}: {e}\")\n",
    "        # We don't write manifests here per your division; fail fast so workflow can surface issue.\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32d5bc9c-16a1-42b4-adef-f1a442e5d447",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if missing_files:\n",
    "    # This usually indicates the staged files were cleaned up or the staging path\n",
    "    # is not accessible from this cluster.\n",
    "    # Fail fast so the workflow stops (downstream cannot proceed without the staged files).\n",
    "    msg = (\n",
    "        \"Some staged files are missing on disk (staging path missing/inaccessible). \"\n",
    "        + \"; \".join([f\"fp={fp} file={fn} path={lp}\" for fp, fn, lp in missing_files])\n",
    "    )\n",
    "    logger.error(msg)\n",
    "    raise FileNotFoundError(msg)\n",
    "\n",
    "if not work_items:\n",
    "    logger.info(\"No work items generated from staged files. Exiting (no-op).\")\n",
    "    dbutils.notebook.exit(\"NO_WORK_ITEMS\")\n",
    "\n",
    "schema = T.StructType(\n",
    "    [\n",
    "        T.StructField(\"file_fingerprint\", T.StringType(), False),\n",
    "        T.StructField(\"file_name\", T.StringType(), False),\n",
    "        T.StructField(\"local_path\", T.StringType(), False),\n",
    "        T.StructField(\"institution_id\", T.StringType(), False),\n",
    "        T.StructField(\"inst_col\", T.StringType(), False),\n",
    "        T.StructField(\"file_size\", T.LongType(), True),\n",
    "        T.StructField(\"file_modified_time\", T.TimestampType(), True),\n",
    "        T.StructField(\"planned_at\", T.TimestampType(), False),\n",
    "    ]\n",
    ")\n",
    "\n",
    "df_plan = spark.createDataFrame(work_items, schema=schema)\n",
    "\n",
    "logger.info(\"Work items summary by file (distinct institutions):\")\n",
    "df_plan.groupBy(\"file_name\").agg(\n",
    "    F.countDistinct(\"institution_id\").alias(\"institution_count\")\n",
    ").orderBy(\"file_name\").show(truncate=False)\n",
    "\n",
    "df_plan.createOrReplaceTempView(\"incoming_plan_rows\")\n",
    "\n",
    "# Idempotent upsert: unique per (file_fingerprint, institution_id)\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    MERGE INTO {PLAN_TABLE_PATH} AS t\n",
    "    USING incoming_plan_rows AS s\n",
    "    ON  t.file_fingerprint = s.file_fingerprint\n",
    "    AND t.institution_id   = s.institution_id\n",
    "    WHEN MATCHED THEN UPDATE SET\n",
    "      t.file_name          = s.file_name,\n",
    "      t.local_path         = s.local_path,\n",
    "      t.inst_col           = s.inst_col,\n",
    "      t.file_size          = s.file_size,\n",
    "      t.file_modified_time = s.file_modified_time,\n",
    "      t.planned_at         = s.planned_at\n",
    "    WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "count_out = df_plan.count()\n",
    "logger.info(\n",
    "    f\"Wrote/updated {count_out} institution work item(s) into {PLAN_TABLE_PATH}.\"\n",
    ")\n",
    "dbutils.notebook.exit(f\"WORK_ITEMS={count_out}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_file_institution_expand",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
