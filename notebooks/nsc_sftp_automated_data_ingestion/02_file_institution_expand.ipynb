{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d24bd56-23f1-486b-94e3-cfb635e262e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Read each *staged* local file (from pending_ingest_queue), detect the institution id column,\n",
    "2. extract unique institution ids, and emit per-institution work items.\n",
    "\n",
    "Constraints:\n",
    " - NO SFTP connection\n",
    " - NO API calls\n",
    " - NO volume writes\n",
    "\n",
    "Output table:\n",
    "- staging_sst_02.default.institution_ingest_plan\n",
    "- (file_fingerprint, file_name, local_path, institution_id, inst_col, file_size, file_modified_time, planned_at)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "679b2064-2a15-4d89-abda-5e9c0148ff61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install pandas python-box pyyaml paramiko\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62608829-5027-4075-a4fc-1e4afc36ef3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from box import Box\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "from helper import CustomLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64156fce-07a6-4eb6-8612-6b29bc06edfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logger = CustomLogger()\n",
    "\n",
    "# Config (kept consistent with prior notebooks)\n",
    "with open(\"gcp_config.yaml\", \"rb\") as f:\n",
    "    cfg = Box(yaml.safe_load(f))\n",
    "\n",
    "CATALOG = \"staging_sst_01\"\n",
    "DEFAULT_SCHEMA = \"default\"\n",
    "\n",
    "QUEUE_TABLE = f\"{CATALOG}.{DEFAULT_SCHEMA}.pending_ingest_queue\"\n",
    "PLAN_TABLE  = f\"{CATALOG}.{DEFAULT_SCHEMA}.institution_ingest_plan\"\n",
    "\n",
    "logger.info(\"Loaded config and initialized logger.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61dd2548-1ed7-4e50-b2c5-3a447d102ec7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def ensure_plan_table():\n",
    "    spark.sql(\n",
    "        f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {PLAN_TABLE} (\n",
    "          file_fingerprint STRING,\n",
    "          file_name STRING,\n",
    "          local_path STRING,\n",
    "          institution_id STRING,\n",
    "          inst_col STRING,\n",
    "          file_size BIGINT,\n",
    "          file_modified_time TIMESTAMP,\n",
    "          planned_at TIMESTAMP\n",
    "        )\n",
    "        USING DELTA\n",
    "        \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4abcbd9-8522-4166-a052-7cea2062338b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def normalize_col(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Same column normalization as the current script.\n",
    "    \"\"\"\n",
    "    name = name.strip().lower()\n",
    "    name = re.sub(r\"[^a-z0-9_]\", \"_\", name)\n",
    "    name = re.sub(r\"_+\", \"_\", name)\n",
    "    name = name.strip(\"_\")\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6374e96c-7cd3-4f14-9ac8-a8183b6a91fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Same hard-coded renames from the current script (kept identical)\n",
    "RENAMES = {\n",
    "    \"attemptedgatewaymathyear1\": \"attempted_gateway_math_year_1\",\n",
    "    \"attemptedgatewayenglishyear1\": \"attempted_gateway_english_year_1\",\n",
    "    \"completedgatewaymathyear1\": \"completed_gateway_math_year_1\",\n",
    "    \"completedgatewayenglishyear1\": \"completed_gateway_english_year_1\",\n",
    "    \"gatewaymathgradey1\": \"gateway_math_grade_y_1\",\n",
    "    \"gatewayenglishgradey1\": \"gateway_english_grade_y_1\",\n",
    "    \"attempteddevmathy1\": \"attempted_dev_math_y_1\",\n",
    "    \"attempteddevenglishy1\": \"attempted_dev_english_y_1\",\n",
    "    \"completeddevmathy1\": \"completed_dev_math_y_1\",\n",
    "    \"completeddevenglishy1\": \"completed_dev_english_y_1\",\n",
    "}\n",
    "\n",
    "INST_COL_PATTERN = re.compile(r\"(?=.*institution)(?=.*id)\", re.IGNORECASE)\n",
    "\n",
    "def detect_institution_column(cols):\n",
    "    \"\"\"\n",
    "    Detect institution id column using the same regex logic as the current script.\n",
    "    Returns the matched column name or None.\n",
    "    \"\"\"\n",
    "    return next((c for c in cols if INST_COL_PATTERN.search(c)), None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16f879d8-8946-4f70-8e36-143ed334d25b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def extract_institution_ids(local_path: str):\n",
    "    \"\"\"\n",
    "    Read staged file with the same parsing approach (pandas read_csv),\n",
    "    normalize/rename columns, detect institution column, return (inst_col, unique_ids).\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(local_path, on_bad_lines=\"warn\")\n",
    "    df = df.rename(columns={c: normalize_col(c) for c in df.columns})\n",
    "    df = df.rename(columns=RENAMES)\n",
    "\n",
    "    inst_col = detect_institution_column(df.columns)\n",
    "    if inst_col is None:\n",
    "        return None, []\n",
    "\n",
    "    # Make IDs robust: drop nulls, strip whitespace, keep as string\n",
    "    series = df[inst_col].dropna()\n",
    "\n",
    "    # Some files store as numeric; normalize to integer-like strings when possible\n",
    "    ids = set()\n",
    "    for v in series.tolist():\n",
    "        # Handle pandas/numpy numeric types\n",
    "        try:\n",
    "            if isinstance(v, (int,)):\n",
    "                ids.add(str(v))\n",
    "                continue\n",
    "            if isinstance(v, float):\n",
    "                # If 323100.0 -> \"323100\"\n",
    "                if v.is_integer():\n",
    "                    ids.add(str(int(v)))\n",
    "                else:\n",
    "                    ids.add(str(v).strip())\n",
    "                continue\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        s = str(v).strip()\n",
    "        if s == \"\" or s.lower() == \"nan\":\n",
    "            continue\n",
    "        # If it's \"323100.0\" as string, coerce safely\n",
    "        if re.fullmatch(r\"\\d+\\.0+\", s):\n",
    "            s = s.split(\".\")[0]\n",
    "        ids.add(s)\n",
    "\n",
    "    return inst_col, sorted(ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87047914-fec0-4f35-b33f-d1b927605d11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ensure_plan_table()\n",
    "\n",
    "# Pull queued staged files (Script 1 output)\n",
    "if not spark.catalog.tableExists(QUEUE_TABLE):\n",
    "    logger.info(f\"Queue table {QUEUE_TABLE} not found. Exiting (no-op).\")\n",
    "    dbutils.notebook.exit(\"NO_QUEUE_TABLE\")\n",
    "\n",
    "queue_df = spark.read.table(QUEUE_TABLE)\n",
    "\n",
    "if queue_df.limit(1).count() == 0:\n",
    "    logger.info(\"pending_ingest_queue is empty. Exiting (no-op).\")\n",
    "    dbutils.notebook.exit(\"NO_QUEUED_FILES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21683394-0bec-42b8-82dd-1a4590519de5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Avoid regenerating plans for files already expanded\n",
    "existing_fp = spark.table(PLAN_TABLE).select(\"file_fingerprint\").distinct() if spark.catalog.tableExists(PLAN_TABLE) else None\n",
    "if existing_fp is not None:\n",
    "    queue_df = queue_df.join(existing_fp, on=\"file_fingerprint\", how=\"left_anti\")\n",
    "\n",
    "if queue_df.limit(1).count() == 0:\n",
    "    logger.info(\"All queued files have already been expanded into institution work items. Exiting (no-op).\")\n",
    "    dbutils.notebook.exit(\"NO_NEW_EXPANSION_WORK\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "540c7880-f14a-4607-979a-856f17066c50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "queued_files = queue_df.select(\n",
    "    \"file_fingerprint\",\n",
    "    \"file_name\",\n",
    "    F.col(\"local_tmp_path\").alias(\"local_path\"),\n",
    "    \"file_size\",\n",
    "    \"file_modified_time\",\n",
    ").collect()\n",
    "\n",
    "logger.info(f\"Expanding {len(queued_files)} staged file(s) into per-institution work items...\")\n",
    "\n",
    "work_items = []\n",
    "missing_files = []\n",
    "\n",
    "for r in queued_files:\n",
    "    fp = r[\"file_fingerprint\"]\n",
    "    file_name = r[\"file_name\"]\n",
    "    local_path = r[\"local_path\"]\n",
    "\n",
    "    if not local_path or not os.path.exists(local_path):\n",
    "        missing_files.append((fp, file_name, local_path))\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        inst_col, inst_ids = extract_institution_ids(local_path)\n",
    "        if inst_col is None:\n",
    "            logger.warning(f\"No institution id column found for file={file_name} fp={fp}. Skipping this file.\")\n",
    "            continue\n",
    "\n",
    "        if not inst_ids:\n",
    "            logger.warning(f\"Institution column found but no IDs present for file={file_name} fp={fp}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        now_ts = datetime.now(timezone.utc)\n",
    "        for inst_id in inst_ids:\n",
    "            work_items.append(\n",
    "                {\n",
    "                    \"file_fingerprint\": fp,\n",
    "                    \"file_name\": file_name,\n",
    "                    \"local_path\": local_path,\n",
    "                    \"institution_id\": inst_id,\n",
    "                    \"inst_col\": inst_col,\n",
    "                    \"file_size\": r[\"file_size\"],\n",
    "                    \"file_modified_time\": r[\"file_modified_time\"],\n",
    "                    \"planned_at\": now_ts,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        logger.info(f\"file={file_name} fp={fp}: found {len(inst_ids)} institution id(s) using column '{inst_col}'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Failed expanding file={file_name} fp={fp}: {e}\")\n",
    "        # We don't write manifests here per your division; fail fast so workflow can surface issue.\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32d5bc9c-16a1-42b4-adef-f1a442e5d447",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if missing_files:\n",
    "    # This usually indicates the cluster changed or /tmp was cleared.\n",
    "    # Fail fast so the workflow stops (downstream cannot proceed without the staged files).\n",
    "    msg = \"Some staged files are missing on disk (likely /tmp cleared or different cluster). \" \\\n",
    "          + \"; \".join([f\"fp={fp} file={fn} path={lp}\" for fp, fn, lp in missing_files])\n",
    "    logger.error(msg)\n",
    "    raise FileNotFoundError(msg)\n",
    "\n",
    "if not work_items:\n",
    "    logger.info(\"No work items generated from staged files. Exiting (no-op).\")\n",
    "    dbutils.notebook.exit(\"NO_WORK_ITEMS\")\n",
    "\n",
    "schema = T.StructType(\n",
    "    [\n",
    "        T.StructField(\"file_fingerprint\", T.StringType(), False),\n",
    "        T.StructField(\"file_name\", T.StringType(), False),\n",
    "        T.StructField(\"local_path\", T.StringType(), False),\n",
    "        T.StructField(\"institution_id\", T.StringType(), False),\n",
    "        T.StructField(\"inst_col\", T.StringType(), False),\n",
    "        T.StructField(\"file_size\", T.LongType(), True),\n",
    "        T.StructField(\"file_modified_time\", T.TimestampType(), True),\n",
    "        T.StructField(\"planned_at\", T.TimestampType(), False),\n",
    "    ]\n",
    ")\n",
    "\n",
    "df_plan = spark.createDataFrame(work_items, schema=schema)\n",
    "df_plan.createOrReplaceTempView(\"incoming_plan_rows\")\n",
    "\n",
    "# Idempotent upsert: unique per (file_fingerprint, institution_id)\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    MERGE INTO {PLAN_TABLE} AS t\n",
    "    USING incoming_plan_rows AS s\n",
    "    ON  t.file_fingerprint = s.file_fingerprint\n",
    "    AND t.institution_id   = s.institution_id\n",
    "    WHEN MATCHED THEN UPDATE SET\n",
    "      t.file_name          = s.file_name,\n",
    "      t.local_path         = s.local_path,\n",
    "      t.inst_col           = s.inst_col,\n",
    "      t.file_size          = s.file_size,\n",
    "      t.file_modified_time = s.file_modified_time,\n",
    "      t.planned_at         = s.planned_at\n",
    "    WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "count_out = df_plan.count()\n",
    "logger.info(f\"Wrote/updated {count_out} institution work item(s) into {PLAN_TABLE}.\")\n",
    "dbutils.notebook.exit(f\"WORK_ITEMS={count_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc228f6a-2fb6-4a76-a573-07f91b0f551f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_file_institution_expand",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
